<!DOCTYPE html>
<html lang="en">
	<head>
		<style>
			body {
				background-color: black;
				color: white;
				font-family: Arial, sans-serif;
				min-width: 100vw;
			}
			div {
				border-radius: 8px;
				background-color: rgb(30, 30, 30);
			}
		</style>
	</head>
	<body>
		<script src="audioEditor.js"></script>
		<script src="audioExporter.js"></script>
		<script src="audioEffects.js"></script>
		<script src="audioLSAC.js"></script>
		<script src="audioFBAC.js"></script>
		<script src="lame.min.js"></script>
		<script src="audioEncoders.unpkg.js"></script>
		<script src="flacjs/libflac.min.js"></script>
		<script src="opus/libopus.wasm.js"></script>
		<script src="modules/cooley-FFT.js"></script>
		Volume (<a id="la" style="width: 100px">100</a>%): <input type="range" id="vol" min="0" max="10" step="0.01" value="1" style="width: 100%"><br>
		Volume Multiplier (<a id="la2" style="width: 100px">100</a>%): <input type="range" id="vol2" min="0" max="10" step="0.01" value="1" style="width: 100%"><br>
		<a id="selectlabel">Selecting A Certain Region?</a> <input type="checkbox" id="select">
		<h1>Audio Editor</h1>
		Current Audio File: <input type="file" id="aud" accept=".wav, .mp3, .ogg, .flac, .aac, .weba, .m4v, .m4a, .mp4, .3gp, .3g2, .webm, .aif, .aiff .lsac, .fbac" multiple><br>
		<button id="which">Decode The Audio File</button> <button id="loadblank">Load Blank Audio</button> <button id="reimp" disabled>Re-import Selected Audio</button>
		<br>
		<div id="frame" style="display: inline-block; width: max-content;"></div>
		<hr>
		<h1>Legal Information</h1>
		<h2>License Information</h2>
		<a>This audio editor is licensed under the MIT License. You can see the license</a> <a href="https://github.com/UnnamedBruh/newer-audio-editor/blob/main/LICENSE">here</a><a>. The MIT License disclaims <i>no warranty</i>, which means that you do not have to replace a copy of this software. The license also disclaims <i>no liability</i>, which means that the copyright holders of this software ("we" or "us") are not responsible for any claims or damages, in spite of any contract or agreement <i>regarding this software</i> stating otherwise.</a><br><br>
		<a>We do not guarantee that this software will remain stable or function as intended without Wi-Fi, or another networking service or technology.</a><br><br>
		<h2>Important / Humorous Information</h2>
		<a>If this software does not function as intended, you can alert</a> <a href="https://www.youtube.com/@UnnamedBruhGD">the original YouTuber</a> <a>, who is the author and copyright holder of this software. The author may or may not respond to your written complaint, with or without notice, at a time that does or does not match your expectations.</a><br>
		<a>If the complaint is networking-related, and <b>does not mention any privacy concerns</b>, the author will either A) simply dismiss it, or B) eventually respond to the complaint(s) whenever they experience emotional displeasure.</a><br><br>
		<a>Thank you for understanding the unintentionally-wordy section above, and your patience.</a>
		<h2>Privacy Policy</h2>
		<a>We value your privacy and are committed to transparency regarding how we handle your information. When you upload files to this project, we do not collect, store, sell, track, monitor, accumulate, hoard, gather, lend, send, transfer or transmit any data associated with these files or your personal information. This includes uploaded files, any data generated from this software, and additional fetching information your browser sends to the original newer-audio-editor website.</a>
		<script>
			const audioContext = new (window.AudioContext || window.webkitAudioContext)();
			const audioInput = document.getElementById("aud"), volume = document.getElementById("vol"), label = document.getElementById("la"), volume2 = document.getElementById("vol2"), label2 = document.getElementById("la2"), selectBox = document.getElementById("select"), selectLabel = document.getElementById("selectlabel"), frame = document.getElementById("frame"), reimp = document.getElementById("reimp");
			let exporters = null;
			let sharedBuffer = null, src = null, whenPlayed = 0, playing = false, timeline = null, timelineBegin = null, timelineEnd = null, highlight = null, timelineBeginNum = 0, timelineEndNum = 0, timelineEndNumCache = 0, timelineBeginAudio = 0, timelineEndAudio = 0, timelineIter = -1, isPressingShift = false, startAt = 0, bbb = 0, ed = null, div = null, data = null, dataSecond = null, skipSample = 480, volumeWarning = true, prevIntervalID = null, xOff, skipSampleUnround = 480;
			let gainNode = audioContext.createGain();
			const interpretFiles = document.getElementById("which");
			const loadBlank = document.getElementById("loadblank");
			let whichInterpret = false;
			selectBox.oninput = function() {
				isPressingShift = selectBox.checked;
			}
			interpretFiles.onclick = function() {
				whichInterpret = !whichInterpret;
				interpretFiles.textContent = whichInterpret ? "Treat File Data As Audio" : "Decode The Audio File";
				audioInput.accept = whichInterpret ? "*" : ".wav, .mp3, .ogg, .flac, .aac, .weba, .m4v, .m4a, .mp4, .3gp, .3g2, .webm, .aif, .aiff, .lsac, .fbac";
			}
			gainNode.connect(audioContext.destination);
			gainNode.gain.value = 1;
			function volumeHandle(volume, label) {
				if (volumeWarning) {
					volumeWarning = false;
					alert("Moving the knob to the right can cause incredibly loud results! Please use the volume sliders responsibly and ethically, as these are very experimental features.");
					volume.value = 1;
				}
				let x = +volume.value;
				if (x > 0.9 && x < 1.1) x = volume.value = 1;
				label.textContent = Math.round(x * 100);
				label.textContent = label.textContent;
				gainNode.gain.value = x * +volume2.value;
				if (x >= +volume.max / 1.1) {
					volume.max *= 10;
					volume.disabled = true;
					document.onmouseup = function() {
						volume.disabled = false;
						document.onmouseup = null;
					}
				}
			}
			volume.oninput = function() {
				volumeHandle(volume, label);
			}
			volume2.oninput = function() {
				volumeHandle(volume2, label2);
			}
			function __(s,m) {
				if (!s) return ""; else return m||":";
			}
			function formatTime(seconds) {
				let str = "";
				let f = "";
				let y = false;
				if (seconds >= 3600) {
					str += __(str) + Math.floor(seconds/3600).toFixed(0).padStart("0", 2);
					seconds = seconds % 3600;
					f += __(f) + "hh";
					y = true;
				}
				if (y || seconds >= 60) {
					str += __(str) + Math.floor(seconds/60).toFixed(0).padStart("0", 2);
					seconds = seconds % 60;
					f += __(f) + "mm";
					y = true;
				}
				if (y || seconds >= 1) {
					str += __(str) + Math.floor(seconds).toFixed(0).padStart("0", 2);
					seconds = seconds % 1;
					f += __(f) + "ss";
					y = true;
				}
				if (y || seconds > 0) {
					str += __(str) + Math.floor(seconds*1000).toFixed(0).padStart("0", 3);
					f += __(f,".") + "ms";
					y = true;
				}
				return str + " (in " + f + " format)";
			}
			function tick() {
				if (timeline && playing) {
					let x = (Date.now() - whenPlayed) / 1000;
					if (sampleSkip === 1) {
						timeline.setAttribute("x", x * sharedBuffer.sampleRate / sampleSkip + bbb - window.pageXOffset);
					} else {
						timeline.setAttribute("x", x * sharedBuffer.sampleRate / sampleSkip + bbb);
					}
					if (timelineEndNum !== 0 && x > timelineEndNumCache) {
						playing = false;
						src.stop();
					}
				}
				requestAnimationFrame(() => tick());
			}
			function playSound() {
				if (!sharedBuffer) return;
				if (src) {
					src.onended = function() {};
					if (playing) {
						try {
							src.stop();
						} catch {};
					}
				}
				src = audioContext.createBufferSource();
				src.buffer = sharedBuffer;
				src.connect(gainNode);
				startAt += timelineBeginNum / div.clientWidth * sharedBuffer.duration;
				src.start(0, startAt);
				whenPlayed = Date.now() - startAt * 1000;
				playing = true;
				if (timeline) timeline.setAttribute("fill", "red");
				src.onended = function() {
					playing = false;
					timeline.setAttribute("fill", "none");
				}
			}
			function generateWaveform(exp) {
				let data2 = ["M 0,100 L 0,100"];
				const len = exp.audioData.length, y = exp.audioData;
				let prev = 0, val;
				let ep = 0.004;
				val = y[0];
				let b = 0;
				for (let i = 0, j = 0; i < len; i = b, j++) {
					b = i + sampleSkip;
					if (Math.abs(prev - val) >= ep) {
						data2.push("L " + j + "," + Math.round(100 + val * 100));
						val = y[b] || 0;
					}
					prev = y[i];
				}
				data2.push("L " + (len / sampleSkip) + ",100");
				return data2.join(" ");
			}
			let filename = "";
			function downloadFile(blob, extension) {
				const link = document.createElement("a");
				link.download = (filename || "exportedAudio") + extension;
				link.href = URL.createObjectURL(blob);
				link.click();
				link.remove();
			}
			function loadDiv(exp, blank = false) {
				if (prevIntervalID !== undefined) clearInterval(prevIntervalID);
				const v = document.createElement("div");
				const x = document.createElementNS("http://www.w3.org/2000/svg", "svg");
				frame.appendChild(v);
				v.appendChild(x);
				x.setAttribute("width", exp[0].audioData.length / sampleSkip);
				v.style.width = (exp[0].audioData.length / sampleSkip) + "px";
				x.setAttribute("height", 200);
				const n = document.createElementNS("http://www.w3.org/2000/svg", "path");
				x.appendChild(n);
				n.setAttribute("fill", "white");
				n.setAttribute("stroke-width", 1);
				if (blank) data = ""; else data = generateWaveform(exp[0]);
				n.setAttribute("d", data);
				v.appendChild(document.createElement("br"));
				const play = document.createElement("button");
				play.textContent = "Play Sound";
				v.appendChild(play);
				timeline = document.createElementNS("http://www.w3.org/2000/svg", "rect");
				timeline.setAttribute("height", 200);
				timeline.setAttribute("width", 2);
				timeline.setAttribute("fill", "none");
				timeline.setAttribute("x", 0);
				timelineBegin = document.createElementNS("http://www.w3.org/2000/svg", "rect");
				timelineBegin.setAttribute("height", 200);
				timelineBegin.setAttribute("width", 2);
				timelineBegin.setAttribute("fill", "none");
				timelineBegin.setAttribute("x", 0);
				timelineEnd = document.createElementNS("http://www.w3.org/2000/svg", "rect");
				timelineEnd.setAttribute("height", 200);
				timelineEnd.setAttribute("width", 2);
				timelineEnd.setAttribute("fill", "none");
				timelineEnd.setAttribute("x", 0);
				highlight = document.createElementNS("http://www.w3.org/2000/svg", "rect");
				highlight.setAttribute("height", 200);
				highlight.setAttribute("width", 2);
				highlight.setAttribute("fill", "none");
				highlight.setAttribute("x", 0);
				x.appendChild(timeline);
				x.appendChild(timelineBegin);
				x.appendChild(timelineEnd);
				x.appendChild(highlight);
				let n2 = null;
				if (exp.length > 1 && confirm("Would you like the second channel to be displayed?")) {
					n2 = document.createElementNS("http://www.w3.org/2000/svg", "path");
					x.appendChild(n2);
					n.setAttribute("fill", "white");
					n2.setAttribute("fill", "red");
					n2.setAttribute("stroke-width", 1);
					n.setAttribute("fill-opacity", 0.5);
					n2.setAttribute("fill-opacity", 0.5);
					if (blank) dataSecond = ""; else dataSecond = generateWaveform(exp[1]);
					n2.setAttribute("d", dataSecond);
					alert("White waveform = left hearing, red waveform = right hearing. Note: If parts of the waveform are a light red, this means the audio track sounds nearly identical for both hearing sides.");
				}
				let bpmMarks = [];
				function restartSelect() {
					timelineIter = -1;
					timelineBegin.setAttribute("fill", "none");
					timelineBegin.setAttribute("x", 0);
					timelineEnd.setAttribute("fill", "none");
					timelineEnd.setAttribute("x", 0);
					highlight.setAttribute("fill", "none");
					highlight.setAttribute("x", 0);
					timelineBeginNum = 0;
					timelineEndNum = 0;
					timelineEndNumCache = 0;
					timelineBeginAudio = 0;
					timelineEndAudio = 0;
				}
				play.onclick = () => {
					startAt = 0;
					bbb = 0;
					playSound();
				}
				function playWithTimeline(eve) {
					let clickX = eve.clientX - (xOff - window.scrollX);
					if (isPressingShift) {
						timelineIter++;
						switch (timelineIter) {
							case 0: {
								timelineBegin.setAttribute("x", clickX);
								timelineBegin.setAttribute("fill", "#00AAFF");
								timelineBeginNum = clickX;
								timelineBeginAudio = timelineBeginNum * sampleSkip;
								break;
							}
							case 1: {
								timelineEnd.setAttribute("x", clickX);
								timelineEnd.setAttribute("fill", "#00AAFF");
								highlight.setAttribute("fill", "#00AAFF");
								highlight.setAttribute("fill-opacity", 0.3);
								timelineEndNum = clickX;
								if (timelineEndNum < timelineBeginNum) {
									const temp = timelineEndNum;
									timelineEndNum = timelineBeginNum;
									timelineBeginNum = temp;
									timelineBeginAudio = timelineBeginNum * sampleSkip;
								}
								timelineEndNumCache = timelineEndNum / div.clientWidth * sharedBuffer.duration;
								timelineEndAudio = timelineEndNum * sampleSkip;
								highlight.setAttribute("x", timelineBeginNum);
								highlight.setAttribute("width", timelineEndNum - timelineBeginNum);
								break;
							}
							case 2: {
								restartSelect();
								break;
							}
						}
						return;
					}
					const relativeX = clickX / x.clientWidth;
					startAt = relativeX * sharedBuffer.duration; // Time in seconds

					if (timelineBeginNum) startAt = 0;
					playSound();
				}
				x.onclick = playWithTimeline;
				const label3 = document.createElement("a");
				const co = document.createElement("input");
				label3.textContent = "View at Sample-Level";
				v.appendChild(label3);
				co.type = "checkbox";
				v.appendChild(co);
				co.oninput = function() {
					if (co.checked) {
						sampleSkip = 1;
						x.setAttribute("width", exp[0].audioData.length);
						v.style.width = exp[0].audioData.length + "px";
					} else {
						sampleSkip = 480;
						if (!data) data = generateWaveform(exp[0]);
						x.setAttribute("width", exp[0].audioData.length / sampleSkip);
						x.style.marginLeft = "0px";
						v.style.width = (exp[0].audioData.length / sampleSkip) + "px";
						n.setAttribute("d", data);
						if (n2) n2.setAttribute("d", dataSecond);
					}
				}
				let count = 0;
				let yu = exp[0].audioData.length;
				window.onkeydown = function(key) {
					if (key.key === " ") {
						key.preventDefault()
						const s = isPressingShift;
						isPressingShift = false;
						playWithTimeline({clientX: v.getBoundingClientRect().x - window.pageXOffset});
						isPressingShift = s;
					}
				}
				prevIntervalID = setInterval(function() {
					xOff = v.getBoundingClientRect().x + window.pageXOffset;
				}, 1000);
				v.appendChild(document.createElement("br"));
				const opt = document.createElement("select");
				const apply = document.createElement("button");
				const applyToSpecific = document.createElement("button");
				const params = document.createElement("div");
				const bpm = document.createElement("button");
				v.appendChild(bpm);
				bpm.textContent = "Label BPM";
				bpm.onclick = function() {
					if (bpmMarks.length > 0) {
						for (const b of bpmMarks) b.remove();
					}
					bpmMarks = [];
					const bpmNum = Number(prompt("What is the amount of beats per minute of this track?") || NaN);
					if (!isNaN(bpmNum)) {
						if (bpmNum > 2400) {
							alert("There are too many BPM.");
							return;
						} else if (bpmNum <= 1) {
							alert("The amount of BPM is too low.");
							return;
						}
						const inc = (60 / bpmNum) * (div.clientWidth / sharedBuffer.duration);
						const data1 = document.createElementNS("http://www.w3.org/2000/svg", "path");
						data1.setAttribute("stroke", "yellow");
						data1.setAttribute("fill", "none");
						data1.setAttribute("width", div.clientWidth);
						data1.setAttribute("height", "200");
						const data2 = document.createElementNS("http://www.w3.org/2000/svg", "path");
						data2.setAttribute("stroke", "orange");
						data1.setAttribute("fill", "none");
						data2.setAttribute("width", div.clientWidth);
						data1.setAttribute("height", "200");
						let data1Data = [], data2Data = [];
						const wid = div.clientWidth;
						for (let i = 0, j = 0; i < wid; i += inc, j++) {
							if (j & 3) data2Data.push(`M${i},0 V200`); else data1Data.push(`M${i},0 V200`);
						}
						data1.setAttribute("d", data1Data.join(" "));
						data2.setAttribute("d", data2Data.join(" "));
						x.appendChild(data1);
						x.appendChild(data2);
						bpmMarks.push(data1, data2);
					}
				}
				v.appendChild(document.createElement("br"));
				const renameFile = document.createElement("button");
				v.appendChild(renameFile);
				renameFile.textContent = "Rename Audio";
				renameFile.onclick = function() {
					filename = prompt("What will be the name of the edited audio?").split(".")[0];
				}
				v.appendChild(document.createElement("br"));
				const metadataOfAudio = document.createElement("pre");
				function updateMetadata() {
					metadataOfAudio.textContent = "Duration: " + formatTime(exporters[0].audioData.length / exporters[0].sampleRate) + "\nSamplerate: " + exporters[0].sampleRate;
				}
				updateMetadata();
				v.appendChild(metadataOfAudio);
				v.appendChild(document.createElement("br"));
				{
					const label = document.createElement("a");
					label.textContent = "Effect To Apply: ";
					let option = document.createElement("option");
					option.textContent = "Please select an effect";
					option.value = "placeholder";
					v.appendChild(label);
					v.appendChild(opt);
					opt.appendChild(option);
					for (let effect of effectsList) {
						option = document.createElement("option");
						option.textContent = effect[0];
						option.value = effect[4];
						opt.appendChild(option);
						if (effect[6] && effect[6] > 1) option.style.color = "gray";
					}
					v.appendChild(params);
					apply.textContent = "Apply";
					v.appendChild(apply);
					applyToSpecific.textContent = "Apply To Specific Channel";
					v.appendChild(applyToSpecific);
					let currentSelected = null, paramElements = null, effect;
					async function applyEffectOnExporter(i) {
						console.log(i);
						const expor = exporters[i];
						const values = [expor, ...paramElements.map((x, i) => {
							if (x.type === "checkbox") {
								return effect[5][i](x.checked);
							} else {
								return effect[5][i](x.value);
							}
						})];
						if (effect[6] > 1) values[0] = exporters;
						console.log(values);
						if (timelineBeginAudio !== 0 && effect[4] !== "trim") values[0] = {audioData: expor.audioData.subarray(timelineBeginAudio, timelineEndAudio), sampleRate: expor.sampleRate};
						const le = (effect[6] > 1 ? values[0][0] : values[0]).audioData.length;
						const exporterT = (effect[6] > 1 ? values[0][0] : values[0]);
						await effects[effect[4]](...values);
						if (timelineBeginAudio !== 0) {
							if (exporterT.audioData.buffer !== expor.audioData.buffer) {
								if (exporterT.audioData.length === timelineEndAudio - timelineBeginAudio) {
									expor.audioData.set(exporterT.audioData, timelineBeginAudio);
								} else {
									const data = new Float32Array(exporterT.audioData.length + expor.audioData.length - le);
									data.set(expor.audioData.subarray(0, timelineBeginAudio), 0);
									data.set(exporterT.audioData, timelineBeginAudio);
									data.set(expor.audioData.subarray(timelineEndAudio, expor.audioData.length), exporterT.audioData.length + timelineBeginAudio);
									expor.audioData = data;
									//timelineEndAudio = values[0].audioData.length - timelineBeginAudio;
								}
							}
						}
					}
					async function applyEffect() {
						if (currentSelected) {
							apply.disabled = true;
							for (let i = 0; i < exporters.length; i++) {
								await applyEffectOnExporter(i);
							}
							if (exporters[1] && exporters[1].audioData.length !== exporters[0].audioData.length) {
								const index = (!exporters[1] || exporters[0].audioData.length > exporters[1].audioData.length) ? 0 : 1;
								const indexInverse = 1 - index;
								effects["blank"](exporters[indexInverse], (exporters[index].audioData.length - exporters[indexInverse].audioData.length) / exporters[index].sampleRate, confirm("It looks like the channels have unique lengths, which would cause unexpected bugs during exporting. Do you want to pad silence at the beginning (Yes/OK) or end (Cancel/No)?") ? "f" : "");
							}
							data = generateWaveform(exp[0]);
							if (n2) dataSecond = generateWaveform(exp[1]);
							sharedBuffer = audioContext.createBuffer(exp.length, exp[0].audioData.length, exp[0].sampleRate);
							const ref = sharedBuffer.getChannelData(0);
							ref.set(exp[0].audioData);
							if (exp.length >= 2) {
								const ref2 = sharedBuffer.getChannelData(1);
								ref2.set(exp[1].audioData);
							}
							x.setAttribute("width", exp[0].audioData.length / sampleSkip);
							v.style.width = (exp[0].audioData.length / sampleSkip) + "px";
							n.setAttribute("d", data);
							if (n2) n2.setAttribute("d", dataSecond);
							apply.disabled = false;
							x.onclick = playWithTimeline; // Reset playing function, just in case browsers like Chrome misbehave every time an effect is applied
							yu = exp[0].audioData.length;
							updateMetadata();
						}
					}
					apply.onclick = async function() {
						await applyEffect();
					}
					applyToSpecific.onclick = async function() {
						const numberIndexes = ["first", "second", "third", "fourth"];
						await applyEffectOnExporter((prompt("Which channel do you want to apply the effect to? (" + (exporters.map((x, i) => {return (i+1) + " = " + numberIndexes[i] + " channel";}).join(", ")) + ")") ?? 1) - 1);
					}
					let previous = 0;
					opt.oninput = function() {
						effect = effectsList.find(x => x[4] === opt.value);
						if (effect[6] && sharedBuffer.numberOfChannels < effect[6]) {
							const d = ["mono", "stereo"];
							alert("That effect doesn't support " + d[sharedBuffer.numberOfChannels + 1] + " audio! Tip: Gray effects mean that they support " + d[effect[6]] + " audio.");
							opt.selectedIndex = previous;
							return;
						}
						currentSelected = effect;
						if (effect) {
							params.innerHTML = effect[1] + "<br>" + effect[2];
							paramElements = new Array(effect[3]);
							for (let i = 0; i < paramElements.length; i++) {
								paramElements[i] = document.getElementById(effect[4] + i);
							}
						} else {
							params.innerHTML = "";
							paramElements = null;
						}
						params.style.width = "100vw";
						previous = opt.selectedIndex;
					}
				}
				v.appendChild(document.createElement("br"));
				let exportBtn = document.createElement("button");
				v.appendChild(exportBtn);
				exportBtn.textContent = "Export as WAV (Original Library)";
				exportBtn.onclick = function() {
					const metadata = {};
					const author = prompt("Who is the author of this audio? (e.g. \"John Smith\") Leave the input blank if you don't want the author to be specified.");

					if (author === "" || author === undefined || author === null) return;
					const name = prompt("What is the name of this audio? (e.g. \"Electro Dance (Techno Mix)\") Leave the input blank if you don't want the name to be specified.");
					const genre = prompt("What is the genre of this audio? (e.g. \"Techno\") Leave the input blank if you don't want the genre to be specified.");
					const notes = prompt("Any notes or comments for this audio? (e.g. \"This audio is super cool\") Leave the input blank if you don't have any notes or comments.");

					const software = prompt("What software did you use to create this audio? (e.g. \"UnnamedBruh's Audio Editor\") If input is \"UAE\" without quotes, this software's name will be appended to the audio metadata.");

					if (author) metadata["IART"] = author;
					if (name) metadata["INAM"] = name;
					if (genre) metadata["IGNR"] = genre;
					if (notes) metadata["ICMT"] = notes;
					if (software) metadata["ISFT"] = software === "UAE" ? "UnnamedBruh's Audio Editor" : software;

					const encodeBits = prompt("What are the exporting settings of the audio? (0 = 8-bit. 1 = 12-bit. 2 = 16-bit. 3 = 24-bit. 4 = 32-bit. 5 = 32-bit float. 6 = 64-bit float. 7 = mu-law. 8 = a-law. 9 = dpcm.)");

					const exporters2 = timelineBeginAudio ? {audioData: exporters[0].audioData.subarray(timelineBeginAudio, timelineEndAudio), sampleRate: exporters[0].sampleRate, bits: exporters[0].bits, encoding: exporters[0].encoding, convertToWav: exporters[0].convertToWav} : exporters[0];
					const oldBits = exporters2.bits;
					let isCompatible = false;
					if (encodeBits == 0) {
						exporters2.bits = 8;
						exporters2.encoding = "pcm";
					} else if (encodeBits == 1) {
						exporters2.bits = 12;
						exporters2.encoding = "pcm";
						isCompatible = confirm("Older commercial software used to export 12-bit WAV audio using this ratio: 3:2 (3 bytes per 2 samples). However, modern audio decoders expect byte-aligned 12-bit WAV audio that use 4:2 (4 bytes per 2 samples).\n\nDo you want byte-aligned audio?");
					} else if (!encodeBits || encodeBits == 2) {
						exporters2.bits = 16;
						exporters2.encoding = "pcm";
					} else if (encodeBits == 3) {
						exporters2.bits = 24;
						exporters2.encoding = "pcm";
					} else if (encodeBits == 4) {
						exporters2.bits = 32;
						exporters2.encoding = "pcm";
					} else if (encodeBits == 5) {
						exporters2.bits = 32;
						exporters2.encoding = "pcmf32";
					} else if (encodeBits == 6) {
						exporters2.bits = 64;
						exporters2.encoding = "pcmf64";
					} else if (encodeBits == 7) {
						exporters2.bits = 8;
						exporters2.encoding = "mulaw";
					} else if (encodeBits == 8) {
						exporters2.bits = 8;
						exporters2.encoding = "alaw";
					} else if (encodeBits == 9) {
						const whichFormat = prompt("Which DPCM audio format do you need for playback? Note: DPCM encoding isn't fully standardized. For example, one system (think NES) assumes a certain format, but another (think Windows) may assume a different one.\n\n0 = NES hardware, 1-bit differences, 4-bit samples\n1 = Xan DPCM, for DOS games by Origin Systems");
						if (whichFormat == 0) {
							exporters2.bits = 1;
							exporters2.encoding = "nesdpcm";
						} else if (whichFormat == 1) {
							exporters2.bits = 8;
							exporters2.encoding = "xandpcm";
						}
					}

					const blob = exporters2.convertToWav(metadata, exporters.length > 1 ? exporters[1].audioData : undefined, isCompatible);

					exporters2.bits = encodeBits;

					downloadFile(blob, ".wav");
				}
				const exportBtn1 = document.createElement("button");
				v.appendChild(exportBtn1);
				exportBtn1.textContent = "Sierra Audio (.SOL or .AUD)";
				exportBtn1.onclick = async function() {
					const encodeBits = prompt("What are the exporting settings of the audio? (0 = 8-bit. 1 = 16-bit. 2 = 8-bit old ADPCM. 3 = 8-bit new ADPCM. 4 = 16-bit new ADPCM.)");

					const exporters2 = timelineBeginAudio ? {audioData: exporters[0].audioData.subarray(timelineBeginAudio, timelineEndAudio), sampleRate: exporters[0].sampleRate, bits: exporters[0].bits, encoding: exporters[0].encoding, convertToWav: exporters[0].convertToWav} : exporters[0];
					const oldBits = exporters2.bits;
					if (encodeBits == 0) {
						exporters2.bits = 8;
						exporters2.encoding = "pcm";
					} else if (!encodeBits || encodeBits == 1) {
						exporters2.bits = 16;
						exporters2.encoding = "pcm";
					} else if (encodeBits == 2) {
						exporters2.bits = 8;
						exporters2.encoding = "dpcmold";
					} else if (encodeBits == 3) {
						exporters2.bits = 8;
						exporters2.encoding = "dpcm";
					} else if (encodeBits == 4) {
						exporters2.bits = 16;
						exporters2.encoding = "dpcm";
					}
					
					const blob = await exporters2.convertToSol(exporters.length > 1 ? exporters[1].audioData : undefined);

					exporters2.bits = encodeBits;

					downloadFile(blob, ".sol");
				}
				const exportBtn2 = document.createElement("button");
				v.appendChild(exportBtn2);
				exportBtn2.textContent = "Export as MP3 (liblamejs)";
				exportBtn2.onclick = async function() {
					function floatTo16BitPCM(float32Array, channels) {
						const len = float32Array.length / channels;
						const buffer = new Int16Array(len);
						let s = 0;
						if (channels === 1) {
							for (let i = 0; i < len; i++) {
								s = float32Array[i];
								buffer[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
							}
						} else {
							for (let i = 0, j = 0; i < len; i++, j+=channels) {
								s = float32Array[j|0];
								buffer[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
							}
						}
						return buffer;
					}

					let sampleRate = exporters[0].sampleRate;
					const kbps = Math.ceil(Number(prompt("What is the KBPS (Kilobits Per Second) of the MP3 audio? (e.g. 8, 16, 32, 64, 80, 96, 128, 168, 192, 256, 320)")) / 16) * 16;
					if (kbps === 0 || isNaN(kbps) || kbps === undefined || kbps === null) return;

					const qual = Math.min(9, Math.max(0, prompt("What is the VBR (Variable BitRate) of the MP3 audio? (between 0 and 9. 0 = best quality and slower encoding, 9 = worst quality and faster encoding, 3 = balanced)") ?? 3));

					const channels = (exporters.length === 2) ? 2 : 1;
					const shouldSplit = channels !== 2 && confirm("Should the same buffer be split into two channels? (If so, the audio may sound 2X lower-quality)");

					const isJoint = (channels === 2 || shouldSplit) ? prompt("Should the encoder use Mid/Side encoding for more efficient audio compression? (0 = No, 1 = Yes, 2 = Yes; swap Mid/Side, 3 = Yes; move into the Side because the Side has the least amount of bits)") : 0;

					const lossTimes = Number(prompt("How many times should the same audio be re-encoded? (for glitch-art audio)") || 1);

					let blob;
					exportBtn2.disabled = true;

					const fixEncodeTable = {32: 0.75, 16: 0.75, 8: 0.75};
					const fixEncodeTable2 = {32: 2.177427725956358, 16: 3.0012627195702253, 128: 1.0891036274839623};
					const fixEncodeTable3 = {128: 1.088449674954518};
					const tttt = fixEncodeTable[kbps] || 1;

					for (let i = 0; i < lossTimes; i++) {
						let buffer = i === 0 ? exporters.map(exp => timelineBeginAudio ? exp.audioData.subarray(timelineBeginAudio, timelineEndAudio) : exp.audioData) : await audioContext.decodeAudioData(await blob.arrayBuffer());
						let altRate = buffer.sampleRate;
						if (i > 0) {
							if (buffer.numberOfChannels === 2) buffer = [buffer.getChannelData(0), buffer.getChannelData(1)]; else buffer = [buffer.getChannelData(0), buffer.getChannelData(0)];
						}
						const mp3Encoder = new lamejs.Mp3Encoder(channels + shouldSplit, i === 0 ? sampleRate : altRate, kbps, qual);

						let mp3Data = [];
						let quantity = channels === 2 || shouldSplit ? ((2 / (kbps/64)) * tttt) : (fixEncodeTable2[kbps] || 1);

						if (isJoint) quantity *= fixEncodeTable3[kbps] || 1;

						let pcmData = [floatTo16BitPCM(buffer[0], quantity)];
						const len = pcmData[0].length;
						if (channels === 1) pcmData = [pcmData[0], pcmData[0]]; else pcmData = [pcmData[0], floatTo16BitPCM(buffer[1], quantity)];
						if (isJoint) {
							if (isJoint == 1) {
								let temp = 0, len = pcmData[0].length;
								const left = pcmData[0], right = pcmData[1];
								for (let i = 0; i < len; i++) {
									temp = left[i];
									left[i] = (left[i] + right[i]) * 0.5;
									right[i] = (temp - right[i]) * 0.5;
								}
							} else if (isJoint == 2) {
								let temp = 0, len = pcmData[0].length;
								const left = pcmData[0], right = pcmData[1];
								for (let i = 0; i < len; i++) {
									temp = right[i];
									right[i] = (left[i] + right[i]) * 0.5;
									left[i] = (left[i] - temp) * 0.5;
								}
							} else {
								let len = pcmData[0].length;
								const left = pcmData[0], right = pcmData[1];
								for (let i = 0; i < len; i++) {
									right[i] = (left[i] + right[i]) * 0.5;
									left[i] = 0;
								}
							}
						}
						const len2 = len - 1152;
						let xx;

						for (let i = 0; i < len2; i += 1152) {
							xx = i > len2 ? len : i + 1152;
							const chunk = [pcmData[0].subarray(i, xx), pcmData[1].subarray(i, xx)];

							// For mono input, pass chunk for both channels if needed
							const mp3Chunk = mp3Encoder.encodeBuffer(chunk[0], chunk[1]);
							mp3Data.push(mp3Chunk);
						}

						mp3Data.push(mp3Encoder.flush());
						blob = new Blob(mp3Data, { type: "audio/mp3" });
					}
					exportBtn2.disabled = false;

					downloadFile(blob, ".mp3");
				}
				const exportBtn3 = document.createElement("button");
				v.appendChild(exportBtn3);
				exportBtn3.textContent = "Export as OGG (Vorbis format, wasm-media-encoders)";
				exportBtn3.onclick = async function() {
					exportBtn.disabled = true;
					try {
						const vbr = Number(prompt("How high can the quality of the audio be? (-1.0 = worst quality, low sizes; 10.0 = best quality, higher sizes; 4.0 = balanced quality, minimal differences, medium sizes; -2.0 = absolute worst, lowest sizes)") ?? 4.0) ?? 4.0;

						let blob;
						const split = exporters.length !== 2 && !!confirm("Should the audio be split into two separate channels?");

						const times = Math.ceil(Math.abs(Number(prompt("How many times would the audio be re-encoded?") || 1) || 1));

						for (let i = 0; i < times; i++) {
							const encoder = await WasmMediaEncoder.createOggEncoder();

							let buffer = i === 0 ? exporters.map(exp => timelineBeginAudio ? exp.audioData.subarray(timelineBeginAudio, timelineEndAudio) : exp.audioData) : await audioContext.decodeAudioData(await blob.arrayBuffer());
							let altRate = buffer.sampleRate;
							if (i > 0) {
								if (buffer.numberOfChannels === 2) buffer = [buffer.getChannelData(0), buffer.getChannelData(1)]; else buffer = [buffer.getChannelData(0)];
							}

							encoder.configure({
								sampleRate: i === 0 ? exporters[0].sampleRate : altRate,
								channels: Math.min(2, buffer.length + split),
								vbrQuality: vbr,
							});
							
							const encodedData = encoder.encode(split || buffer.length === 2 ? [buffer[0], buffer[1] || buffer[0]] : [buffer[0]]);
							const encodedCopy = new Uint8Array(encodedData).buffer;
							const finalData = encoder.finalize();

							blob = new Blob([encodedCopy, finalData], { type: "audio/ogg" });
						}

						downloadFile(blob, ".ogg");
					} catch (err) {
						alert("It seems like the OGG Vorbis codec is supported, but not yet available. READ BELOW:\n\nA network is required to export your audio into the OGG format. The encoder itself is stored in an online module server (UNPKG.com), then loaded into this audio editor. (This means your audio track or other invaluable information isn't tracked, or sent to any server; just encoded directly in your browser!)");
						console.log(err.message, err.stack);
					}
					exportBtn3.disabled = false;
				}
				const exportBtn4 = document.createElement("button");
				v.appendChild(exportBtn4);
				exportBtn4.textContent = "Export as OGG (Opus format, libopusjs)";
				exportBtn4.onclick = async function() {
					exportBtn4.disabled = true;
					function floatTo16BitPCM(float32Array, channels) {
						const len = float32Array[0].length;
						const buffer = new Int16Array(len * channels);
						let s = 0;
						if (channels === 1) {
							for (let i = 0; i < len; i++) {
								s = float32Array[0][i];
								buffer[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
							}
						} else {
							for (let i = 0, j = 0; i < len; i++, j+=channels) {
								s = float32Array[0][i];
								buffer[j] = s < 0 ? s * 0x8000 : s * 0x7fff;
								s = float32Array[1][i];
								buffer[j + 1] = s < 0 ? s * 0x8000 : s * 0x7fff;
							}
						}
						return buffer;
					}
// Simple Ogg-Opus muxer (browser compatible) (I DID NOT WRITE ANY OF THE OGG MUXING LOGIC!!! GPT-5.0 Non-mini/Mini, and Claude Sonnet 4.5 DID THIS!)
// - Expects: packets = Array<Uint8Array> (raw Opus packets, in order)
// - Options:
//		sampleRate: number (default 48000)
//		channels: number (default 2)
//		serial: 32-bit stream serial (random by default)
//		pageSizeLimit: approx max body size per page (defaults chosen for decent packing)
//		getPacketSamples(packet): function(Uint8Array) -> number (samples per channel)
//			 If not provided, default assumes frames of 20ms (960 samples @48kHz) and
//			 parses simple TOC frame-count rules (good if your encoder uses 20ms frames).
// Returns: Blob (type 'audio/ogg')
//
// References:
//	- Ogg framing: RFC 3533 / xiph.org docs.
//	- Opus in Ogg: RFC 7845.
//	- Opus packet TOC: RFC 6716 (used for a basic frame-count calculation).
//
// This is intentionally minimal; production muxers do additional checks and
// better packing heuristics.

// Full browser-ready Ogg-Opus muxer (minimal, RFC-compliant) — corrected paging logic
function muxOpusPacketsToOgg(packets, opts = {}) {
	const {
		sampleRate = 48000,
		channels = 2,
		serial = (Math.random() * 0xffffffff) >>> 0,
		pageSizeLimit = 65000,
		getPacketSamples = defaultGetPacketSamples(20, sampleRate),
		vendor = "libopusjs-muxer",
		debug = true
	} = opts;

	// --- Helper functions (unchanged) ---
	const concatUint8 = (chunks) => {
		const total = chunks.reduce((sum, c) => sum + c.length, 0);
		const out = new Uint8Array(total);
		let offset = 0;
		for (const c of chunks) { out.set(c, offset); offset += c.length; }
		return out;
	};

	function write32LE(buf, offset, value) {
		const dv = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
		dv.setUint32(offset, value >>> 0, true);
	}
	function write64LE(buf, offset, value) {
		const dv = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
		const v = BigInt(value);
		dv.setUint32(offset, Number(v & 0xffffffffn), true);
		dv.setUint32(offset + 4, Number((v >> 32n) & 0xffffffffn), true);
	}

	// CRC32 table (unchanged)
	const CRC32 = (() => {
		const table = new Uint32Array(256);
		for (let i = 0; i < 256; i++) {
			let r = i;
			for (let j = 0; j < 8; j++) {
				r = (r & 1) ? (0xEDB88320 ^ (r >>> 1)) : (r >>> 1);
			}
			table[i] = r >>> 0;
		}
		return (buf) => {
			let crc = 0xffffffff;
			for (let i = 0; i < buf.length; i++) crc = (crc >>> 8) ^ table[(crc ^ buf[i]) & 0xff];
			return (crc ^ 0xffffffff) >>> 0;
		};
	})();

	// OpusHead (unchanged)
	function createOpusHead() {
		const buf = new Uint8Array(19);
		buf.set(new TextEncoder().encode("OpusHead"), 0);
		buf[8] = 1;			  // version
		buf[9] = channels;	   // channel count
		buf[10] = 0x38; buf[11] = 0x01; // pre-skip 312 samples
		buf[12] = sampleRate & 0xff; buf[13] = (sampleRate >> 8) & 0xff;
		buf[14] = (sampleRate >> 16) & 0xff; buf[15] = (sampleRate >> 24) & 0xff;
		buf[16] = buf[17] = 0;   // gain
		buf[18] = 0;			 // mapping family
		return buf;
	}

	// OpusTags (unchanged)
	function createOpusTags(vendorStr = vendor, comments = {}) {
		const vendorBytes = new TextEncoder().encode(vendorStr);
		const userComments = Object.entries(comments).map(([k,v]) => `${k}=${v}`);
		const commentBytes = userComments.map(s => new TextEncoder().encode(s));

		let totalLen = 8 + 4 + vendorBytes.length + 4;
		for (const b of commentBytes) totalLen += 4 + b.length;

		const buf = new Uint8Array(totalLen);
		const dv = new DataView(buf.buffer);
		let offset = 0;
		buf.set(new TextEncoder().encode("OpusTags"), offset); offset += 8;
		dv.setUint32(offset, vendorBytes.length, true); offset += 4;
		buf.set(vendorBytes, offset); offset += vendorBytes.length;
		dv.setUint32(offset, commentBytes.length, true); offset += 4;
		for (const b of commentBytes) { dv.setUint32(offset, b.length, true); offset += 4; buf.set(b, offset); offset += b.length; }
		return buf;
	}

	// --- New robust page builder ---
	const pages = [];
	let pageSeq = 0;
	let granule = 0n; // absolute samples (per-channel) up to last completed packet
	const MAX_SEGMENTS = 255;

	function buildAndPushPage(segmentTable, bodyParts, pageStartedWithContinuation, isBOS, isEOS) {
		const header = new Uint8Array(27 + segmentTable.length);
		header.set(new TextEncoder().encode("OggS"), 0);
		header[4] = 0; // version
		let flags = 0x00;
		if (pageStartedWithContinuation) flags |= 0x01;
		if (isBOS) flags |= 0x02;
		if (isEOS) flags |= 0x04;
		header[5] = flags;

		// granule must be set before CRC calculation; pass the current granule (already updated for completed packets)
		write64LE(header, 6, granule);

		write32LE(header, 14, serial);
		write32LE(header, 18, pageSeq++);
		write32LE(header, 22, 0); // CRC placeholder

		header[26] = segmentTable.length;
		header.set(segmentTable, 27);

		const pageBody = concatUint8(bodyParts);
		// compute CRC over header(with CRC=0) + body, then write CRC into header and push
		const fullPage = concatUint8([header, pageBody]);
		const crc = CRC32(fullPage);
		write32LE(header, 22, crc);

		pages.push(concatUint8([header, pageBody]));
	}

	// --- Write BOS pages (OpusHead + OpusTags) ---
	buildAndPushPage(
		new Uint8Array([createOpusHead().length]), // temporary single-segment trick doesn't matter here because we put OpusHead as a single packet below
		[createOpusHead()],
		false, /*continued*/ true, /*BOS*/ false /*EOS*/
	);

	buildAndPushPage(
		(function(){ // OpusTags may be larger than 255 segments but almost never; treat it as one full packet normally
			const t = createOpusTags(vendor, {});
			const segs = [];
			let off = 0;
			while (off < t.length) { const take = Math.min(255, t.length - off); segs.push(take); off += take; }
			return new Uint8Array(segs);
		})(),
		[createOpusTags(vendor, {})],
		false, /*continued*/ false /*BOS*/ , false /*EOS*/
	);

	// --- Audio packet to pages logic (robust) ---
	let pktIndex = 0;
	let pktOffset = 0; // byte offset inside current packet (for continuation)
	while (pktIndex < packets.length) {
		// start a new page
		const segs = [];
		const bodyParts = [];
		let bodySize = 0;
		const pageStartedWithContinuation = pktOffset !== 0;
		let segmentsCount = 0;

		// We'll remember whether any packet *completed* on this page so we can update granule accordingly.
		// granule already contains samples up to the last completed packet before this page.
		// When a packet completes on this page we add its samples to granule immediately so final granule reflects last-complete.
		let anyCompletionOnThisPage = false;

		while (segmentsCount < MAX_SEGMENTS && bodySize < pageSizeLimit && pktIndex < packets.length) {
			const pkt = packets[pktIndex];
			const remaining = pkt.length - pktOffset;
			const toTake = Math.min(255, remaining);

			// If adding this segment would exceed the pageSizeLimit, break.
			if (bodySize + toTake > pageSizeLimit) {
				// If bodySize is zero and toTake > pageSizeLimit, we still must put at least one segment (even if > limit) — but toTake <= 255 so okay.
				break;
			}

			// append segment and body chunk
			segs.push(toTake);
			bodyParts.push(pkt.subarray(pktOffset, pktOffset + toTake));
			bodySize += toTake;
			segmentsCount += 1;
			pktOffset += toTake;

			if (pktOffset === pkt.length) {
				// packet completed on this page — increment granule by its sample count
				const samples = BigInt(getPacketSamples(pkt));
				granule += samples;
				anyCompletionOnThisPage = true;

				// move to next packet
				pktIndex += 1;
				pktOffset = 0;
			} else {
				// packet still continues to next page
				// we must stop if segmentsCount reached 255 (outer loop checks), otherwise the outer while will continue
				// but we must break if we've filled the page (bodySize >= pageSizeLimit) — outer loop checks
			}
		}
		// Decide EOS: only true for the final page after we've processed all packets (and no remaining partial)
		const isEOS = (pktIndex >= packets.length && pktOffset === 0);
		// Special rule: if this page contains NO completed packets, granule must be set to the previous granule value (which it already is)
		// build segment table Uint8Array
		const segmentTable = new Uint8Array(segs);
		// push page
		buildAndPushPage(segmentTable, bodyParts, pageStartedWithContinuation, false, isEOS);
		// Continue; the loop will start next page with pktOffset possibly non-zero (continuation)
	}
	if (debug) console.log(`Created ${pages.length} pages, final granule: ${granule}`);
	return new Blob(pages, { type: "audio/ogg" });
}
// --- Default packet sample counter ---
function defaultGetPacketSamples(frameMs = 20, sampleRate = 48000) {
	const frameSamples = Math.round(sampleRate * (frameMs / 1000));
	return function(packet) {
		if (!packet || packet.length === 0) return 0;
		const toc = packet[0];
		const code = toc & 0x03;
		let frames = 1;
		if (code === 1 || code === 2) frames = 2;
		else if (code === 3 && packet.length >= 2) frames = packet[1] & 0x3f || 1;
		return frames * frameSamples;
	};
}
					try {
						const vbr = Number(prompt("How high can the bitrate of the audio be? (6.0 = worst quality, big differences, low sizes; 512.0 = best quality, nearly-neglible differences, higher sizes; 256.0 = balanced quality, minimal differences, medium sizes)") ?? 256.0) ?? 256.0;
						const voiceOpt = confirm("Should the audio encoder optimize for voices in the audio?");
						let blob;
						const split = exporters.length !== 2 && !!confirm("Should the audio be split into two separate channels?");

						const times = Number(prompt("How many times would the audio be re-encoded?") || 1) || 1;

						for (let i = 0; i < times; i++) {
							let buffer = i === 0 ? exporters.map(exp => timelineBeginAudio ? exp.audioData.subarray(timelineBeginAudio, timelineEndAudio) : exp.audioData) : await audioContext.decodeAudioData(await blob.arrayBuffer());
							let altRate = buffer.sampleRate;
							if (i > 0) {
								if (buffer.numberOfChannels === 2) buffer = [buffer.getChannelData(0), buffer.getChannelData(1)]; else buffer = [buffer.getChannelData(0)];
							}
// Rest of encoding logic is implemented by GPT-5.0 Mini.
		// Prepare channels
		const bufferF = split || buffer.length === 2 ? [buffer[0], buffer[1] || buffer[0]] : [buffer[0]];
		const channels = bufferF.length;

							const sampleRate = altRate || exporters[0].sampleRate; // Except this line, I added this.

		// Create encoder
		const encoder = new Encoder(channels, sampleRate, vbr, 20, voiceOpt);
		const arr = [];

		// Frame-slice
		const frameSize = sampleRate * 20 / 1000; // 20ms @ 48kHz | Except this line, this altered modified by me.
		const totalSamples = bufferF[0].length;

		for (let pos = 0; pos + frameSize <= totalSamples; pos += frameSize) {
			const frame = bufferF.map(ch => ch.subarray(pos, pos + frameSize));
			const pcm = floatTo16BitPCM(frame, channels);
			encoder.input(pcm);

			// Drain packets
			let pkt;
			while ((pkt = encoder.output()) != null) {
									arr.push(pkt);
								}
							}

							console.log("Packets:", arr);

							// Mux into OGG
							blob = muxOpusPacketsToOgg(arr, {
								sampleRate,
								channels
							});
							encoder.destroy();
						}

						downloadFile(blob, ".ogg");
					} catch (err) {
						alert("It seems like the OGG Opus codec is supported, but not yet available. READ BELOW:\n\nA network is required to export your audio into the OGG format. The encoder itself is stored in an online module server (UNPKG.com), then loaded into this audio editor. (This means your audio track or other invaluable information isn't tracked, or sent to any server; just encoded directly in your browser!)");
						console.log(err.message, err.stack);
					}
					exportBtn4.disabled = false;
				}
				const exportBtn5 = document.createElement("button");
				v.appendChild(exportBtn5);
				exportBtn5.textContent = "Export as FLAC (libflac.js)";
				exportBtn5.onclick = async function() { // This function was written entirely by Claude Sonnet 4.5. I made some modifications to the original code.
					exportBtn5.disabled = true;
					try {
						// Setup for collecting encoded data
						const encBuffer = [];

						// Callback to receive encoded FLAC data
						function write_callback_fn(buffer) {
							// Copy the buffer immediately (same issue as with OGG Vorbis!)
							const copy = new Uint8Array(buffer);
							encBuffer.push(copy);
						}

						// Create encoder
						const CHANNELS = exporters.length;
						const SAMPLERATE = exporters[0].sampleRate;
						const BPS = Number(prompt("What is the bits per sample of the FLAC audio? (Although 8-bit is nonstandard, it is supported across many systems.)") ?? 24)>>3<<3; // 24-, 16- or 8-bit encoding
						const COMPRESSION = Number(prompt("What is the quality of the audio? (In other terms, how aggressive is the lossless compression? 0 = fastest compression, largest file, 8 = slower compression, smaller file, 5 = balanced compression, balanced file)") ?? 5);
						const VERIFY = false;
						const BLOCK_SIZE = 0; // 0 = let encoder decide

						const junkMode = (BPS !== 16 || confirm("Do you want your FLAC to not have any garbage data?")) ? 1 : 0;

						const flac_encoder = Flac.create_libflac_encoder(
							SAMPLERATE,
							CHANNELS,
							BPS,
							COMPRESSION,
							(timelineBeginAudio ? (timelineEndAudio - timelineBeginAudio) : exporters[0].audioData.length) * (2 - junkMode), // total_samples (0 = unknown)
							VERIFY,
							BLOCK_SIZE
						);

						// Initialize encoder with write callback
						const initStatus = Flac.init_encoder_stream(
							flac_encoder,
							write_callback_fn
						);

						if (initStatus !== 0) {
							throw new Error("FLAC encoder initialization failed");
						}

						// Convert Float32 to 24-bit integers (packed as Int32Array)
						const audioData = exporters.map(i => timelineBeginAudio ? 
							i.audioData.subarray(timelineBeginAudio, timelineEndAudio) :
							i.audioData);

						let encodedStatus = false;

						if (BPS === 24) {
							const hh = audioData[0].length * audioData.length;
							const int24Buffer = new Int32Array(Math.ceil(hh / 4096) * 4096); // FLAC encoders often expect 4096 for block sizes.
							if (audioData.length === 2) {
								const aa = audioData[0];
								const ab = audioData[1];
								for (let i = 0, j = 0; i < hh; i += 2, j++) {
									int24Buffer[i] = (aa[j] * 8388607) | 0;
									int24Buffer[i + 1] = (ab[j] * 8388607) | 0;
								}
							} else {
								const aa = audioData[0];
								for (let i = 0; i < aa.length; i++) {
									int24Buffer[i] = (aa[i] * 8388607) | 0;
								}
							}

							encodedStatus = Flac.FLAC__stream_encoder_process_interleaved(
								flac_encoder,
								int24Buffer,
								audioData[0].length
							);
						} else if (BPS === 8) {
							const hh = audioData[0].length * audioData.length;
							const int8Buffer = new Int16Array(Math.ceil(hh / 4096) * 4096); // FLAC encoders often expect 4096 for block sizes.
							if (audioData.length === 2) {
								const aa = audioData[0];
								const ab = audioData[1];
								for (let i = 0, j = 0; i < hh; i += 2, j++) {
									int8Buffer[i] = (aa[j] * 127) | 0;
									int8Buffer[i + 1] = (ab[j] * 127) | 0;
								}
							} else {
								const aa = audioData[0];
								for (let i = 0; i < aa.length; i++) {
									int8Buffer[i] = (aa[i] * 127) | 0;
								}
							}

							encodedStatus = Flac.FLAC__stream_encoder_process_interleaved(
								flac_encoder,
								int8Buffer,
								audioData[0].length
							);
						} else if (BPS === 16) {
							let int16Buffer = new Int16Array();
							const len = audioData[0].length * audioData.length * 2;
							if (!junkMode) int16Buffer = new Int16Array(len * 2); else int16Buffer = new Int32Array(audioData[0].length * audioData.length);
							if (junkMode) {
								if (audioData.length === 2) {
									const aa = audioData[0];
									const ab = audioData[1];
									for (let i = 0, j = 0; i < int16Buffer.length; i += 2, j++) {
										int16Buffer[i] = (aa[j] * 32767) | 0;
										int16Buffer[i + 1] = (ab[j] * 32767) | 0;
									}
								} else {
									const aa = audioData[0];
									for (let i = 0; i < aa.length; i++) {
										int16Buffer[i] = (aa[i] * 32767) | 0;
									}
								}
							} else {
								if (audioData.length === 2) {
									const aa = audioData[0];
									const ab = audioData[1];
									for (let i = 0, j = 0; i < len; i += 4, j++) {
										int16Buffer[i] = (aa[j] * 32767) | 0;
										int16Buffer[i + 1] = (ab[j] * 32767) | 0;
										int16Buffer[i + 2] = int16Buffer[i];
										int16Buffer[i + 3] = int16Buffer[i + 1];
									}
								} else {
									const aa = audioData[0];
									for (let i = 0, j = 0; i < aa.length; i++, j += 2) {
										int16Buffer[j] = (aa[i] * 32767) | 0;
										int16Buffer[j + 1] = int16Buffer[j];
									}
								}
							}

							encodedStatus = Flac.FLAC__stream_encoder_process_interleaved(
								flac_encoder,
								int16Buffer,
								audioData[0].length
							);
						}

						if (!encodedStatus) {
							throw new Error("FLAC encoding failed");
						}

						// Finish encoding
						const finishStatus = Flac.FLAC__stream_encoder_finish(flac_encoder);

						if (!finishStatus) {
							throw new Error("FLAC finalization failed");
						}

						// Clean up
						Flac.FLAC__stream_encoder_delete(flac_encoder);

						// Combine all encoded chunks
						const totalLength = encBuffer.reduce((sum, chunk) => sum + chunk.length, 0);
						const flacData = new Uint8Array(totalLength);
						let offset = 0;
						for (const chunk of encBuffer) {
							flacData.set(chunk, offset);
							offset += chunk.length;
						}

						// Download
						const blob = new Blob([flacData], { type: "audio/flac" });
						downloadFile(blob, ".flac");
					} catch (err) {
						alert("The FLAC encoding failed: " + err.message);
						console.error(err);
					}
					exportBtn5.disabled = false;
				}
				/*exportBtn = document.createElement("button");
				v.appendChild(exportBtn);
				exportBtn.textContent = "Export as LSAC (LoSsy Audio Codec)";
				exportBtn.onclick = function() {
					const blob = encodeLSAC(exporters[0].audioData, exporters[0].sampleRate);

					const link = document.createElement("a");
					link.download = "exportedAudio.lsac";
					link.href = URL.createObjectURL(blob);
					link.click();
					link.remove();
				}
				exportBtn = document.createElement("button");
				v.appendChild(exportBtn);
				exportBtn.textContent = "Export as FBAC (Frame-Based Audio Codec)";
				exportBtn.onclick = function() {
					const blob = encodeFBAC(exporters[0].audioData, exporters[0].sampleRate);

					const link = document.createElement("a");
					link.download = "exportedAudio.fbac";
					link.href = URL.createObjectURL(blob);
					link.click();
					link.remove();
				}*/
				window.onscroll = function() {
					apply.style.marginLeft = window.scrollX + "px";
					opt.style.marginLeft = apply.style.marginLeft;
					params.style.marginLeft = apply.style.marginLeft;
					selectLabel.style.marginLeft = apply.style.marginLeft;
					metadataOfAudio.style.marginLeft = apply.style.marginLeft;
					renameFile.style.marginLeft = apply.style.marginLeft;
					selectLabel.style.wordWrap = "keep-all";
					selectLabel.style.whiteSpace = "nowrap";
					if (window.scrollX > window.innerWidth - selectLabel.getBoundingClientRect().width) selectBox.style.marginLeft = apply.style.marginLeft; else selectBox.style.marginLeft = "";
					if (sampleSkip === 1) {
						count++;
						if (count < 4) return;
						count = 0;
						let data2 = ["M 0,100 L 0,100"];
						let xn = window.scrollX;
						x.setAttribute("width", window.innerWidth);
						x.style.marginLeft = (xn - xOff) + "px";
						const len = Math.min(xn - xOff + window.innerWidth, yu), y = exp[0].audioData;
						let prev = 0, val = y[0];
						let ep = 0.004;
						for (let i = xn, j = 0; i < len; i++, j++) {
							if (Math.abs(prev - val) >= ep) {
								data2.push("L " + j + "," + Math.round(100 + val * 100));
								prev = val;
							}
							val = y[i];
						}
						data2.push("L " + (len / sampleSkip) + ",100");
						n.setAttribute("d", data2.join(" "));
						if (n2) {
							data2 = ["M 0,100 L 0,100"];
							const y = exp[1].audioData;
							let prev = 0, val = y[0];
							for (let i = xn, j = 0; i < len; i++, j++) {
								if (Math.abs(prev - val) >= ep) {
									data2.push("L " + j + "," + Math.round(100 + val * 100));
									prev = val;
								}
								val = y[i];
							}
							data2.push("L " + (len / sampleSkip) + ",100");
							n2.setAttribute("d", data2.join(" "));
						}
					}
				}
				effects["trim"] = function(ex) {
					if (timelineBeginAudio) ex.audioData = ex.audioData.subarray(timelineBeginAudio, timelineEndAudio); else alert("You have not selected any portion of the audio yet.");
				}
				effects["trimout"] = function(ex) {
					if (timelineBeginAudio) ex.audioData = new Float32Array([]); else alert("You have not selected any portion of the audio yet.");
				}
				ed = v;
				div = x;
			}

			effectsList.push([
				"Trim",
				"Removes all of the contents that is not selected by the blue selection box.",
				'',
				0,
				"trim",
				[]
			]);
			effectsList.push([
				"Trim Out",
				"Removes the contents that are selected by the blue selection box.",
				'',
				0,
				"trimout",
				[]
			]);

			function processSharedBuff(channel, channelNum, isProcessed) {
				if (sharedBuffer.numberOfChannels === 4) {
					const which = confirm("Do you want the surround sound audio to be converted into mono (Cancel/No) or stereo (OK/Yes)?");
					if (which) {
						const newBuffer = audioContext.createBuffer(2, sharedBuffer.length, sharedBuffer.sampleRate);
						const lenBuffer = sharedBuffer.length;
						const channel1 = newBuffer.getChannelData(0);
						const channel2 = newBuffer.getChannelData(1);
						const channel1S = sharedBuffer.getChannelData(0);
						const channel2S = sharedBuffer.getChannelData(1);
						const channel3S = sharedBuffer.getChannelData(2);
						const channel4S = sharedBuffer.getChannelData(3);
						const leftorright = confirm("Okay! Do you want to store the front left and right sides (Cancel/No), or all sides including the front left, front right, back left and back right sides (OK/Yes)?");
						if (leftorright) {
							for (let i = 0; i < lenBuffer; i++) {
								channel1[i] = (channel1S[i] + channel3S[i]) * 0.5;
								channel2[i] = (channel2S[i] + channel4S[i]) * 0.5;
							}
						} else {
							for (let i = 0; i < lenBuffer; i++) {
								channel1[i] = channel1S[i];
								channel2[i] = channel2S[i];
							}
						}
						sharedBuffer = newBuffer;
						channel = sharedBuffer.getChannelData(channelNum & 1);
					} else {
						const newBuffer = audioContext.createBuffer(1, sharedBuffer.length, sharedBuffer.sampleRate);
						const lenBuffer = sharedBuffer.length;
						const channel1 = newBuffer.getChannelData(0);
						const channel1S = sharedBuffer.getChannelData(0);
						const channel2S = sharedBuffer.getChannelData(1);
						const channel3S = sharedBuffer.getChannelData(2);
						const channel4S = sharedBuffer.getChannelData(3);
						const leftorright = confirm("Okay! Do you want to store the left and right sides (Cancel/No), or all sides including the left, right, front and back sides (OK/Yes)?");
						if (leftorright) {
							for (let i = 0; i < lenBuffer; i++) {
								channel1[i] = (channel1S[i] + channel2S[i] + channel3S[i] + channel4S[i]) * 0.25;
							}
								} else {
							for (let i = 0; i < lenBuffer; i++) {
								channel1[i] = (channel1S[i] + channel2S[i]) * 0.5;
							}
						}
						sharedBuffer = newBuffer;
						channel = sharedBuffer.getChannelData(0);
					}
				} else if (sharedBuffer.numberOfChannels === 8) {
					const open = confirm("The editor will assume that this surround sound audio follows specific conventions. This may not be important, but it will play a role in rendering audio into stereo. You may view the page in another tab if you press OK or Yes.");
					const which = confirm("Do you want the 7.1 (or 8.1) surround sound audio to be converted into mono (Cancel/No) or stereo (OK/Yes)?");
					if (which) {
						const newBuffer = audioContext.createBuffer(2, sharedBuffer.length, sharedBuffer.sampleRate);
						const lenBuffer = sharedBuffer.length;
						const channel1 = newBuffer.getChannelData(0);
						const channel2 = newBuffer.getChannelData(1);
						const channel1S = sharedBuffer.getChannelData(0);
						const channel2S = sharedBuffer.getChannelData(1);
						const channel3S = sharedBuffer.getChannelData(2);
						const channel4S = sharedBuffer.getChannelData(3);
						const channel5S = sharedBuffer.getChannelData(4);
						const channel6S = sharedBuffer.getChannelData(5);
						const channel7S = sharedBuffer.getChannelData(6);
						const channel8S = sharedBuffer.getChannelData(7);
						const leftorright = confirm("Okay! Do you want to keep all the centered channels (OK/Yes), or do you want to discard them (Cancel/No)?");
						const xy = 1/5.4;
						if (leftorright) {
							for (let i = 0; i < lenBuffer; i++) {
								channel1[i] = (channel1S[i]*0.75 + channel3S[i] + channel4S[i]*2 + channel5S[i]*0.65 + channel7S[i]) * xy;
								channel2[i] = (channel2S[i]*0.75 + channel3S[i] + channel4S[i]*2 + channel6S[i]*0.65 + channel8S[i]) * xy;
							}
						} else {
							for (let i = 0; i < lenBuffer; i++) {
								channel1[i] = (channel1S[i]*0.75 + channel5S[i]*0.65 + channel7S[i]) * 0.4166666666666667;
								channel2[i] = (channel2S[i]*0.75 + channel6S[i]*0.65 + channel8S[i]) * 0.4166666666666667;
							}
						}
						sharedBuffer = newBuffer;
						channel = sharedBuffer.getChannelData(channelNum & 1);
					} else {
						const newBuffer = audioContext.createBuffer(1, sharedBuffer.length, sharedBuffer.sampleRate);
						const lenBuffer = sharedBuffer.length;
						const channel1 = newBuffer.getChannelData(0);
						const channel1S = sharedBuffer.getChannelData(0);
						const channel2S = sharedBuffer.getChannelData(1);
						const channel3S = sharedBuffer.getChannelData(2);
						const channel4S = sharedBuffer.getChannelData(3);
						const channel5S = sharedBuffer.getChannelData(4);
						const channel6S = sharedBuffer.getChannelData(5);
						const channel7S = sharedBuffer.getChannelData(6);
						const channel8S = sharedBuffer.getChannelData(7);
						const leftorright = confirm("Okay! Do you want to keep all the centered channels (OK/Yes), or do you want to discard them (Cancel/No)?");
						if (leftorright) {
							for (let i = 0; i < lenBuffer; i++) {
								channel1[i] = (channel1S[i] + channel2S[i] + channel3S[i]*2 + channel4S[i]*2 + channel5S[i] + channel6S[i] + channel7S[i] + channel8S[i]) * 0.1;
							}
						} else {
							for (let i = 0; i < lenBuffer; i++) {
								channel1[i] = (channel1S[i] + channel2S[i] + channel5S[i] + channel6S[i] + channel7S[i] + channel8S[i]) * 0.16666666666666666666;
							}
						}
						sharedBuffer = newBuffer;
						channel = sharedBuffer.getChannelData(0);
					}
					if (open) window.open('https://unnamedbruh.github.io/newer-audio-editor/surroundsoundconventions-81.html', '_blank');
				}

				if (sharedBuffer.numberOfChannels === 2) {
					if (confirm("Should the current channel be subtracted by another? (If you want to only edit the selected channel, press Cancel/No)")) {
						const channelOtherNum = sharedBuffer.numberOfChannels - channelNum - 1;
						const len = channel.length;
						const channelOther = sharedBuffer.getChannelData(channelOtherNum);
						const choice = Number(prompt("Okay then! Which formula do you want to choose? (0 = channel" + channelNum + " - otherChannel | 1 = channel" + channelNum + " * 2 - otherChannel | 2 = channel" + channelNum + " - (otherChannel - channel" + channelNum + ") / 2 | 3 = 2 but otherChannel" + channelNum + " * 2 | 4 = channel" + channelNum + " + otherChannel)"));
						if (choice === 0) {
							for (let i = 0; i < len; i++) {
								channel[i] -= channelOther[i];
							}
						} else if (choice === 1) {
							for (let i = 0; i < len; i++) {
								channel[i] = channelOther[i] - channel[i];
							}
						} else if (choice === 2) {
							for (let i = 0; i < len; i++) {
								channel[i] -= Math.abs(channelOther[i] - channel[i]);
							}
						} else if (choice === 3) {
							for (let i = 0; i < len; i++) {
								channel[i] -= channelOther[i] * 2 - channel[i];
							}
						} else if (choice === 3.5) {
							for (let i = 0; i < len; i++) {
								channel[i] -= channelOther[i] - channel[i] * 2;
							}
						} else if (choice === 4) {
							for (let i = 0; i < len; i++) {
								channel[i] = (channel[i] + channelOther[i]) * 0.5;
							}
						}
						isProcessed = true;
					}
				}
				return [channel,isProcessed];
			}

			let oldSharedBufferFor = null;

			function copyBuffer(buff) {
				const copy = audioContext.createBuffer(buff.numberOfChannels, buff.length, buff.sampleRate);
				for (let i = 0; i < buff.numberOfChannels; i++) {
					copy.copyToChannel(buff.getChannelData(i), i);
				}
				return copy;
			}

			audioInput.oninput = async function(eve) {
				if (eve.target.files.length === 0) return;
				let firstFile = eve.target.files[0];
				let secondFile = eve.target.files[1];
				let isCorrect;
				function swapFiles() {
					const temp = secondFile;
					secondFile = firstFile;
					firstFile = temp;
				}
				if (secondFile) {
					isCorrect = confirm("It seems like you selected two audio files. Is this correct?");
					if (isCorrect) alert("*This editor cannot interpret the second file as raw audio because the maintainer of this editor has not implemented this feature... yet." + (whichInterpret ? " Not that the audio won't be decoded." : ""));
					if (!confirm("Which file is the one you want to edit?\nOK = " + firstFile.name.trim() + "\nCancel (or X) = " + secondFile.name.trim())) {
						swapFiles();
					}
				}
				if (!whichInterpret && (firstFile.name.endsWith(".lsac") || firstFile.name.endsWith(".fbac"))) {
					try {
						exporters = null;
						if (ed) {
							ed.innerHTML = "";
							ed.remove();
							ed = null;
						}
						data = null;
						sampleData = null;
						sampleSkip = 480;
						let result = 0;
						if (firstFile.name.endsWith(".lsac")) result = await decodeLSAC(firstFile); else result = await decodeFBAC(firstFile);
						sharedBuffer = audioContext.createBuffer(1, result.samples.length, result.sampleRate);
						const len = result.samples.length;
						const bufferData = sharedBuffer.getChannelData(0);
						const samples = result.samples;
						for (let i = 0; i < len; i++) {
							bufferData[i] = samples[i];
						}
						exporters = [new AudioExporter(result.samples, result.sampleRate, 1, 32)];
						loadDiv(exporters[0]);
					} catch (err) {
						alert("The audio could not be properly loaded! Due to unhelpful strip-aways from viewing metadata, we cannot provide much info on what exactly failed.");
						console.log(err.message + err.stack);
					}
				} else {
					const fileReader = new FileReader();
					fileReader.onload = async function(event) {
						try {
							exporters = null;
							if (ed) {
								ed.innerHTML = "";
								ed.remove();
								ed = null;
							}
							data = null;
							sampleData = null;
							sampleSkip = 480;
							if (whichInterpret) {
								const data = event.target.result;
								const type = Number(prompt("How will the audio be interpreted? (0 = integers, 1 = floats)"));
								const bits = Math.max(prompt("What is the bits per sample of the audio?") >> 3, 1);
								sharedBuffer = audioContext.createBuffer(1, data.byteLength / (bits*2), prompt("What is the samplerate of the audio?") || 48000);
								const channel = sharedBuffer.getChannelData(0);
								const len = sharedBuffer.length;
								const buffer = new Uint8Array(data);
								if (type === 0) {
									if (bits === 1) {
										const bufferInt = new Int8Array(data);
										for (let i = 0; i < len; i++) {
											channel[i] = bufferInt[i] / 0x80;
										}
									} else if (bits === 2) {
										for (let i = 0, j = 0; i < len; i += bits, j++) {
											channel[j] = ((buffer[i + 1] << 8) | buffer[i]) / 0x8000 - 0.5;
										}
									} else if (bits === 3) {
										for (let i = 0, j = 0; i < len; i += bits, j++) {
											channel[j] = ((buffer[i + 2] << 16) | (buffer[i + 1] << 8) | buffer[i]) / 0x800000 - 0.5;
										}
									} else if (bits === 4) {
										for (let i = 0, j = 0; i < len; i += bits, j++) {
											channel[j] = ((buffer[i + 3] << 24) | (buffer[i + 2] << 16) | (buffer[i + 1] << 8) | buffer[i]) / 0x80000000 - 0.5;
										}
									}
								} else if (type === 1) {
									const s = Math.sign;
									if (bits === 4) {
										const f = new Float32Array(data.slice(0, data.byteLength >> 2 << 2));
										const lenF = f.length;
										let x;
										for (let i = 0; i < lenF; i++) {
											x = f[i];
											channel[i] = (x === Infinity ? 1 : (x === -Infinity ? -1 : (isNaN(x) ? 0 : x))) || 0;
											channel[i] = channel[i] > 1 ? 1 : (channel[i] < -1 ? -1 : channel[i]);
										}
									} else if (bits === 8) {
										const f = new Float64Array(data.slice(0, data.byteLength >> 3 << 3));
										const lenF = f.length;
										let x;
										for (let i = 0; i < lenF; i++) {
											x = f[i];
											channel[i] = (x === Infinity ? 1 : (x === -Infinity ? -1 : (isNaN(x) ? 0 : x))) || 0;
											channel[i] = channel[i] > 1 ? 1 : (channel[i] < -1 ? -1 : channel[i]);
										}
									}
								}
								oldSharedBufferFor = copyBuffer(sharedBuffer);
								exporters = [new AudioExporter(channel, sharedBuffer.sampleRate, 1, 32)];
								loadDiv(exporters);
								return;
							}
							sharedBuffer = await audioContext.decodeAudioData(event.target.result);
							if (isCorrect) {
								alert("The first audio file was decoded.");
								const fileReader2 = new FileReader();
								let secondSharedBuffer;
								await new Promise(function(resolve) {
									try {
										fileReader2.onload = async function(eve) {
											try {
												secondSharedBuffer = await audioContext.decodeAudioData(eve.target.result);
											} catch {
												alert("Looks like the second audio file has failed to be decoded! However, you can still edit the main audio.");
											}
											if (secondSharedBuffer && confirm("Would you like to layer both audio files together?")) {
												let inway = Number(prompt("Alright! Which way should the audio editor layer the audio files together? (0 = audioFile1 + audioFile2, 1 = audioFile1 - audioFile2, 2 = audioFile2 - audioFile1") || 0);
												if (sharedBuffer.sampleRate === secondSharedBuffer.sampleRate && sharedBuffer.numberOfChannels === secondSharedBuffer.numberOfChannels) {
													let len;
													if (sharedBuffer.length < secondSharedBuffer.length) {
														len = secondSharedBuffer.length;
														const oldShared = sharedBuffer;
														sharedBuffer = audioContext.createBuffer(oldShared.numberOfChannels, len, oldShared.sampleRate);
														sharedBuffer.copyToChannel(oldShared.getChannelData(0), 0);
														if (sharedBuffer.numberOfChannels >= 2) sharedBuffer.copyToChannel(oldShared.getChannelData(1), 1);
													} else if (secondSharedBuffer.length < sharedBuffer.length) {
														len = sharedBuffer.length;
														const oldShared = secondSharedBuffer;
														secondSharedBuffer = audioContext.createBuffer(oldShared.numberOfChannels, len, oldShared.sampleRate);
														secondSharedBuffer.copyToChannel(oldShared.getChannelData(0), 0);
														if (sharedBuffer.numberOfChannels >= 2) secondSharedBuffer.copyToChannel(oldShared.getChannelData(1), 1);
													} else len = sharedBuffer.length;
													if (inway === 0) {
														for (let i = 0; i < secondSharedBuffer.numberOfChannels; i++) {
															const pointer1 = sharedBuffer.getChannelData(i);
															const pointer2 = secondSharedBuffer.getChannelData(i);
															for (let j = 0; j < len; j++) {
																pointer1[j] += pointer2[j];
															}
														}
													} else if (inway === 1 || inway === 2 || inway === 3 || inway === 4) {
														if (inway === 3 || inway === 4) {
															for (let i = 0; i < secondSharedBuffer.numberOfChannels; i++) {
																const pointer = (inway === 3 ? sharedBuffer.getChannelData(i) : secondSharedBuffer.getChannelData(i));
																for (let j = 0; j < len; j++) {
																	pointer[j] *= 0.5;
																}
															}
															inway -= 2;
														}
														for (let i = 0; i < secondSharedBuffer.numberOfChannels; i++) {
															let pointer1;
															let pointer2;
															if (inway === 1) {
																pointer1 = sharedBuffer.getChannelData(i);
																pointer2 = secondSharedBuffer.getChannelData(i);
															} else {
																pointer2 = sharedBuffer.getChannelData(i);
																pointer1 = secondSharedBuffer.getChannelData(i);
															}
															for (let j = 0; j < len; j++) {
																pointer1[j] -= pointer2[j];
															}
														}
													}
												} else alert("Looks like the audio files' metadata doesn't exactly match each other well! You can still edit the main audio, though.");
											}
											resolve();
										}
										fileReader2.readAsArrayBuffer(secondFile);
									} catch {
										alert("Looks like something went wrong in the mixing process! However, you can still edit the main audio.");
										resolve();
									}
								});
							}
							oldSharedBufferFor = copyBuffer(sharedBuffer);
							const channelNum = Math.max(1, Math.min(+prompt("Which channel should be imported for editing? Pick a channel from 1 to " + sharedBuffer.numberOfChannels + ". (Editing multiple channels at once is now officially an option due to the maintainer's goals! However, this only works on stereo audio, not surround-sound.)") || 1, sharedBuffer.numberOfChannels)) - 1;
							let channel = sharedBuffer.getChannelData(channelNum);
							let isProcessed = false;
							if (sharedBuffer.numberOfChannels > 2 && confirm("Do you wish to select only that channel?")) {
								const newBuffer = audioContext.createBuffer(1, sharedBuffer.length, sharedBuffer.sampleRate);
								newBuffer.copyToChannel(channel, 0);
								sharedBuffer = newBuffer;
							} else {const x = processSharedBuff(channel, channelNum, isProcessed);isProcessed = x[1];channel = x[0];}
							const stereo = sharedBuffer.numberOfChannels >= 2 && !isProcessed && confirm("Do you want the final audio to be stereo?");
							if (sharedBuffer.numberOfChannels > 1 && !stereo) { // Normalize playback, since only one channel is being loaded. If other channels are playing while one is selected, it can suddenly sound "mono" if an effect is applied, which confuses the user
								for (let i = 0; i < sharedBuffer.numberOfChannels; i++) {
									if (i === channelNum) continue;
									const silentBuffer = sharedBuffer.getChannelData(i);
									if (silentBuffer) silentBuffer.set(channel);
								}
							}
							if (stereo) exporters = [new AudioExporter(channel, sharedBuffer.sampleRate, 1, 32), new AudioExporter(sharedBuffer.getChannelData(1 - channelNum), sharedBuffer.sampleRate, 1, 32)];
							else exporters = [new AudioExporter(channel, sharedBuffer.sampleRate, 1, 32)];
							loadDiv(exporters);
							reimp.disabled = false;
							filename = firstFile.name.split(".")[0];
						} catch (err) {
							alert("The audio could not be properly loaded! We couldn't gather any info on what exactly failed, so the best we can say is: try selecting another audio file, or using another browser.");
							console.log(err.message + err.stack);
						}
					}
					fileReader.readAsArrayBuffer(firstFile);
				}
			}
			loadBlank.onclick = function() {
				exporters = null;
				if (ed) {
					ed.innerHTML = "";
					ed.remove();
					ed = null;
				}
				data = null;
				sampleData = null;
				sampleSkip = 480;
				sharedBuffer = audioContext.createBuffer(1 + confirm("Will the blank audio be mono (1 channel, select \"Cancel\") or stereo (2 channels, select \"OK\" or \"Yes\")?"), (Math.round(Math.abs(Number(prompt("How long is the blank audio (in seconds)?")))) || 1) * 48000, 48000);
				const stereo = sharedBuffer.numberOfChannels === 2;
				if (stereo) exporters = [new AudioExporter(sharedBuffer.getChannelData(0), sharedBuffer.sampleRate, 1, 32), new AudioExporter(sharedBuffer.getChannelData(1), sharedBuffer.sampleRate, 1, 32)];
				else exporters = [new AudioExporter(sharedBuffer.getChannelData(0), sharedBuffer.sampleRate, 1, 32)];
				loadDiv(exporters, true);
				reimp.disabled = true;
			}
			reimp.onclick = function() {
				let x = copyBuffer(oldSharedBufferFor);
				sharedBuffer = oldSharedBufferFor;
				oldSharedBufferFor = x;
				exporters = null;
				if (ed) {
					ed.innerHTML = "";
					ed.remove();
					ed = null;
				}
				data = null;
				sampleData = null;
				sampleSkip = 480;
				const channelNum = Math.max(1, Math.min(+prompt("Which channel should be imported for editing? Pick a channel from 1 to " + sharedBuffer.numberOfChannels + ". (Editing multiple channels at once is now officially an option due to the maintainer's goals! However, this only works on stereo audio, not surround-sound.)") || 1, sharedBuffer.numberOfChannels)) - 1;
				let channel = sharedBuffer.getChannelData(channelNum);
				let isProcessed = false;
				if (sharedBuffer.numberOfChannels > 2 && confirm("Do you wish to select only that channel?")) {
					const newBuffer = audioContext.createBuffer(1, sharedBuffer.length, sharedBuffer.sampleRate);
					newBuffer.copyToChannel(channel, 0);
					sharedBuffer = newBuffer;
				} else {const x = processSharedBuff(channel, channelNum, isProcessed);isProcessed = x[1];channel = x[0];}
				const stereo = sharedBuffer.numberOfChannels >= 2 && !isProcessed && confirm("Do you want the final audio to be stereo?");
				if (sharedBuffer.numberOfChannels > 1 && !stereo) {
					for (let i = 0; i < sharedBuffer.numberOfChannels; i++) {
						if (i === channelNum) continue;
						const silentBuffer = sharedBuffer.getChannelData(i);
						if (silentBuffer) silentBuffer.set(channel);
					}
				}
				if (stereo) exporters = [new AudioExporter(channel, sharedBuffer.sampleRate, 1, 32), new AudioExporter(sharedBuffer.getChannelData(1 - channelNum), sharedBuffer.sampleRate, 1, 32)];
				else exporters = [new AudioExporter(channel, sharedBuffer.sampleRate, 1, 32)];
				loadDiv(exporters);
			}
			tick();
		</script>
	</body>
</html>
