<!DOCTYPE html>
<html lang="en">
	<head>
		<style>
			body {
				background-color: black;
				color: white;
				font-family: Arial, sans-serif;
				min-width: 100vw;
			}
			div {
				border-radius: 8px;
				background-color: rgb(30, 30, 30);
			}
			.red{color:red}.orange{color:orange}.yellow{color:yellow}.green{color:green}.blue{color:blue}.purple{color:purple}.white{color:white}.gray{color:gray}.grey{color:gray}.pink{color:pink}.magenta{color:magenta}
			.title,.large,.big,.header,.h,.h1,.h2,.h3{font-size:28px;}.subtitle,.medium,.h4,.h5,.h6{font-size:21px;}.intro{color:yellow;font-weight:bold;}.highlight{background-color:yellow;}.huge,.mega,.larger,.largest,.bigger,.biggest,.colossal{font-size:36px;}.small{font-size:10px;}.tiny{font-size:6px;}.emphasis{font-weight:bold}.warning{background-color:yellow;color:black}.thought{color:gray;font-style:italic}.aside{color:gray;font-size:10px;}.flashback{color:rgb(255,243,163);}
			.hidden,.nodisplay,.none,.invisible,.noopaque{display:none}.left,.alignleft,.alignmentleft,.leftside{text-align:left;width:50%}.right,.alignright,.alignmentright,.rightside{text-align:right;width:50%}
			.fadein{opacity:0;animation:fadeintransition 1s forwards;}@keyframes fadeintransition{to{opacity:1;}}.fadeout{opacity:1;animation:fadeouttransition 2s forwards;}@keyframes fadeouttransition{to{opacity:0;}}
			.slideinleft,.slideleftin,.slidefromleft{margin-right:130%;animation:slideleftin 1.5s forwards;}@keyframes slideleftin{to{margin-right:0%;}}.slideinright,.sliderightin,.slidefromright{margin-left:130%;animation:sliderightin 1.5s forwards;}@keyframes sliderightin{to{margin-left:0%;}}
			.grow,.growin,.growlarge,.growbig{font-size:0px;animation:growin 1s forwards;}@keyframes growin{to{font-size:14px;}}.shrink,.shrinkout,.shrinkin,.shrinksmall,.shrinktiny{font-size:14px;animation:shrinkout 1.5s forwards;}@keyframes shrinkout{to{font-size:0px;}}
			.blink,.wink,.switch,.onandoff,.onnoff,.onoff,.on-and-off{animation:blinkingtext 0.9s infinite;}@keyframes blinkingtext{0%{opacity:1;}70%{opacity:0;}100%{opacity:1;}}.flicker{animation:flickeringtext 4.5s infinite;}@keyframes flickeringtext{0%{opacity:1;}7%{opacity:0;}12%{opacity:1;}18%{opacity:0;}20%{opacity:1;}23%{opacity:0;}29%{opacity:1;}35%{opacity:0;}44%{opacity:1;}50%{opacity:0;}51%{opacity:1;}54%{opacity:0;}55%{opacity:1;}61%{opacity:0;}68%{opacity:1;}69%{opacity:0;}73%{opacity:1;}83%{opacity:0;}88%{opacity:1;}91%{opacity:0;}93%{opacity:1;}96%{opacity:0;}100%{opacity:1;}}
			.flash,.flashbang,.lobotomy{background-color:rgba(255,255,255,1);color:rgb(255,255,255);animation:flashout 0.6s forwards;}@keyframes flashout{to{background-color:rgba(0,0,0,0);}}
			.shadow,.silhouette,.absorb,.absorblight{text-shadow:-5px -5px 5px 0px #636363;}.rainbow,.colorwheel,.hue,.huerotate,.lgbtq{animation:rainbow 3.5s infinite;color:rgb(255,0,0);}@keyframes rainbow{0%{color:rgb(255,0,0);}10%{color:rgb(255,128,0);}25%{color:rgb(255,255,0);}40%{color:rgb(0,200,0);}55%{color:rgb(0,128,128);}70%{color:rgb(0,0,255);}85%{color:rgb(255,0,255);}}
			.arial,.arialnormal,.normalarial,.fontarial,.arialfont{font-family:Arial,sans-serif}.arialblack,.blackarial,.fontarialblack,.arialblackfont{font-family:Arial Black,sans-serif}.helvetica,.helveticanormal,.normalhelvetica,.fonthelvetica,.helveticafont{font-family:Helvetica,sans-serif}.sans-serif,.sansserif,.fontsans-serif,.fontsansserif,.sans-seriffont,.sansseriffont{font-family:sans-serif}
			.ghost,.ghast,.ghostly,.ghastly,.transparent,.translucent,.seethrough,.seebehind,.lowopaque,.lowopaqueness,.low-opaque,.low-opaqueness,.fog,.foggy{animation:ghost 1.7s forwards}@keyframes ghost{0%,100%{opacity:0.2},70%{opacity:0.3}}
			.shrinkheader{animation:shrinkout 1.5s forwards;}
		</style>
		<title>Newer Audio Editor (Searadare)</title>
	</head>
	<body>
		<script>const originalFetchFunction = fetch;</script>
		<script src="audioEditor.js"></script>
		<script src="audioExporter.js"></script>
		<script src="audioEffects.js"></script>
		<script src="lame.min.js"></script>
		<script src="audioEncoders.unpkg.js"></script>
		<script src="flacjs/libflac.min.js"></script>
		<script src="opus/libopus.wasm.js"></script>
		<script src="srt/srt-parser.js"></script>
		<script src="srt/sub-parser.js"></script>
		<script src="srt/vtt-parser.js"></script>
		<script src="asaf_codec.min.js"></script>
		Volume (<a id="la" style="width: 100px">100</a>%): <input type="range" id="vol" min="0" max="10" step="0.01" value="1" style="width: 100%"><br>
		Volume Multiplier (<a id="la2" style="width: 100px">100</a>%): <input type="range" id="vol2" min="0" max="10" step="0.01" value="1" style="width: 100%"><br>
		<a id="selectlabel">Selecting A Certain Region?</a> <input type="checkbox" id="select"> <button id="startsel" disabled>Select Start Of Audio</button> <button id="endsel" disabled>Select End Of Audio</button>
		<h1 id="disappearheader">Searadare (Newer Audio Editor)</h1>
		Current Audio File: <input type="file" id="aud" accept=".wav, .mp3, .ogg, .flac, .aac, .weba, .m4v, .m4a, .mp4, .3gp, .3g2, .webm, .aif, .aiff, .lsac, .fbac" multiple><br>
		Load Audio File From URL: <input type="url" id="audurl" title="Add multiple URLs by separating them with commas ,"><br>
		<button id="which">Decode The Audio File</button> <button id="loadblank">Load Blank Audio</button> <button id="reimp" disabled>Re-import Selected Audio</button> <button id="subtitle" disabled>Import Subtitles</button> <button id="micrecord">Import Recording From Microphone</button>
		<br>
		<div id="frame" style="display: inline-block; width: max-content;"></div>
		<hr id="b">
		<h1>Legal Information</h1>
		<h2>License Information</h2>
		<a>This audio editor is licensed under the MIT License. You can see the license</a> <a href="https://github.com/UnnamedBruh/newer-audio-editor/blob/main/LICENSE">here</a><a>. The MIT License disclaims <i>no warranty</i>, which means that you do not have to replace a copy of this software. The license also disclaims <i>no liability</i>, which means that the copyright holders of this software ("we" or "us") are not responsible for any claims or damages, in spite of any contract or agreement <i>regarding this software</i> stating otherwise.</a><br><br>
		<a>We do not guarantee that this software will remain stable or function as intended without Wi-Fi, or another networking service or technology.</a><br><br>
		<h2>Important / Humorous Information</h2>
		<a>If this software does not function as intended, you can alert</a> <a href="https://www.youtube.com/@UnnamedBruhGD">the original YouTuber</a> <a>, who is the author and copyright holder of this software. The author may or may not respond to your written complaint, with or without notice, at a time that does or does not match your expectations.</a><br>
		<a>If the complaint is networking-related, and <b>does not mention any privacy concerns</b>, the author will either A) simply dismiss it, or B) eventually respond to the complaint(s) whenever they experience emotional displeasure.</a><br><br>
		<a>Thank you for understanding the unintentionally-wordy section above, and your patience.</a>
		<h2>Privacy Policy</h2>
		<a>We value your privacy and are committed to transparency regarding how we handle your information. When you upload files to this project, we do not collect, store, sell, track, monitor, accumulate, hoard, gather, lend, send, transfer or transmit any data associated with these files or your personal information. This includes uploaded files, any data generated from this software, and additional fetching information your browser sends to the original newer-audio-editor website.</a>
		<div id="subtitles" style="display: none; background-color: black; color: white; font-size: 14px; font-family: sans-serif; position: absolute; top: 70vh; left: 0px; width: 100%; height: min-content; text-align: center;"></div>
		<script>
			const audioContext = new (window.AudioContext || window.webkitAudioContext)();
			const audioInput = document.getElementById("aud"), volume = document.getElementById("vol"), label = document.getElementById("la"), volume2 = document.getElementById("vol2"), label2 = document.getElementById("la2"), selectBox = document.getElementById("select"), selectLabel = document.getElementById("selectlabel"), frame = document.getElementById("frame"), reimp = document.getElementById("reimp"), importSubtitles = document.getElementById("subtitle"), subtitlesDiv = document.getElementById("subtitles"), recordMicrophone = document.getElementById("micrecord"), selectStart = document.getElementById("startsel"), selectEnd = document.getElementById("endsel"), loadFromURL = document.getElementById("audurl");
			let exporters = null;
			let sharedBuffer = null, src = null, whenPlayed = 0, playing = false, timeline = null, timelineBegin = null, timelineEnd = null, highlight = null, timelineBeginNum = 0, timelineEndNum = 0, timelineEndNumCache = 0, timelineBeginAudio = 0, timelineEndAudio = 0, timelineIter = -1, isPressingShift = false, startAt = 0, bbb = 0, ed = null, div = null, data = null, dataSecond = null, skipSample = 480, volumeWarning = true, prevIntervalID = null, xOff, skipSampleUnround = 480;
			let gainNode = audioContext.createGain();
			const interpretFiles = document.getElementById("which");
			const loadBlank = document.getElementById("loadblank");
			let whichInterpret = false;
			let canUseSpaceKey = true;
			selectBox.oninput = function() {
				isPressingShift = selectBox.checked;
				selectStart.disabled = !selectBox.checked;
				selectEnd.disabled = selectStart.disabled;
			}
			interpretFiles.onclick = function() {
				whichInterpret = !whichInterpret;
				interpretFiles.textContent = whichInterpret ? "Treat File Data As Audio" : "Decode The Audio File";
				audioInput.accept = whichInterpret ? "*" : ".wav, .mp3, .ogg, .flac, .aac, .weba, .m4v, .m4a, .mp4, .3gp, .3g2, .webm, .aif, .aiff, .lsac, .fbac";
			}
			gainNode.connect(audioContext.destination);
			gainNode.gain.value = 1;
			function volumeHandle(volume, volume2, label) {
				if (volumeWarning) {
					volumeWarning = false;
					alert("Moving the knob to the right can cause incredibly loud results! Please use the volume sliders responsibly and ethically, as these are very experimental features.");
					volume.value = 1;
				}
				let x = +volume.value;
				if (x > 0.9 && x < 1.1) x = volume.value = 1;
				if (x < 0.001) volume.max = 10;
				label.textContent = Math.round(x * 100);
				label.textContent = label.textContent;
				gainNode.gain.value = x * +volume2.value;
				if (x >= +volume.max / 1.1) {
					volume.max *= 10;
					volume.disabled = true;
					document.onmouseup = function() {
						volume.disabled = false;
						volume2.disabled = false;
						document.onmouseup = null;
					}
				}
			}
			volume.oninput = function() {
				volumeHandle(volume, volume2, label);
			}
			volume2.oninput = function() {
				volumeHandle(volume2, volume, label2);
			}
			function __(s,m) {
				if (!s) return ""; else return m||":";
			}
			function formatTime(seconds) {
				let str = "";
				let f = "";
				let y = false;
				if (seconds >= 3600) {
					str += __(str) + Math.floor(seconds/3600).toFixed(0).padStart(2, "0");
					seconds = seconds % 3600;
					f += __(f) + "hh";
					y = true;
				}
				if (y || seconds >= 60) {
					str += __(str) + Math.floor(seconds/60).toFixed(0).padStart(2, "0");
					seconds = seconds % 60;
					f += __(f) + "mm";
					y = true;
				}
				if (y || seconds >= 1) {
					str += __(str) + Math.floor(seconds).toFixed(0).padStart(2, "0");
					seconds = seconds % 1;
					f += __(f) + "ss";
					y = true;
				}
				if (y || seconds > 0) {
					str += __(str,".") + Math.floor(seconds*1000).toFixed(0).padStart(3, "0");
					f += __(f,".") + "SSS";
					seconds = (seconds * 1000) % 1;
					y = true;
				}
				if (y || seconds > 0) {
					str += __(str,".") + Math.floor(seconds*1000).toFixed(0).padStart(3, "0");
					f += __(f,".") + "μμμ";
					seconds = (seconds * 1000) % 1;
					y = true;
				}
				if (y || seconds > 0) {
					str += __(str,".") + Math.floor(seconds*1000).toFixed(0).padStart(3, "0");
					f += __(f,".") + "NNN";
				}
				return str + " (in " + f + " format)";
			}
			function generateWaveform(exp) {
				let data2 = ["M 0,100 L 0,100"];
				const len = exp.audioData.length, y = exp.audioData;
				let prev = 0, val, prevVal=0;
				let ep = 0.004;
				val = prevVal = y[0];
				let b = 0;
				let m = 0;
				for (let i = 0, j = 0, jm = -1; i < len; i = b, j++, jm++) {
					b = i + skipSample;
					if (Math.abs(prev - val) >= ep) {
						if (m !== jm) data2.push("L " + jm + "," + Math.round(100 + prevVal * 100));
						data2.push("L " + j + "," + Math.round(100 + val * 100));
						val = y[b] || 0;
						m = j;
					}
					prevVal = prev;
					prev = y[i];
				}
				data2.push("L " + (len / skipSample) + ",100");
				return data2.join(" ");
			}
			let filename = "";
			function downloadFile(blob, extension) {
				const link = document.createElement("a");
				link.download = (filename || "exportedAudio") + extension;
				link.href = URL.createObjectURL(blob);
				link.click();
				link.remove();
			}
			let subtitlesToDisplay = [];
			let readyToDisplaySubtitles = false;
			let currentSubtitle = 0;
			let numberOfSubtitles = 0;
			let currentSubtitleS = null;
			let tickCache = 0, progress = 0;
			let subtitleType = 0;
			let subtitlesText = [];
			function findSubtitleIndex(p, offset = 0) {
				if (p === 0) return 0;
				if (subtitleType === 0) {
					for (let i = offset; i < subtitlesToDisplay.length; i++) {
						const x = subtitlesToDisplay[i];
						if (p <= x.end) return i;
					}
					return subtitlesToDisplay.length;
				} else if (subtitleType === 1) {
					for (let i = offset; i < subtitlesToDisplay.subtitles.length; i++) {
						const x = subtitlesToDisplay.subtitles[i];
						if (p <= x.end) return i;
					}
					return subtitlesToDisplay.subtitles.length;
				}
			}
			function tick(play) {
				if (timeline && playing) {
					progress = (Date.now() - whenPlayed) * 0.001;
					timeline.setAttribute("x", progress * tickCache);
					if (timelineEndNum && progress > timelineEndNumCache) {
						playing = false;
						try{src.stop();}catch{}
						play.textContent = "Play Sound";
					}
					if (readyToDisplaySubtitles&&(subtitleType===0?subtitlesToDisplay.length:subtitlesToDisplay.subtitles.length)) {
						if (!currentSubtitleS) {
							readyToDisplaySubtitles = false;
						} else {
							if (progress >= currentSubtitleS.start) {
								if (numberOfSubtitles < 4) {
									switch (subtitleType) {
										case 0: {
											const text = document.createElement("pre");
											text.textContent = currentSubtitleS.text;
											subtitlesDiv.appendChild(text);
											numberOfSubtitles++;
											subtitlesDiv.style.display = "";
											text.style.overflowWrap = "break-word";
											text.style.wordBreak = "break-word";
											text.style.whiteSpace = "pre-wrap";
											const index = subtitlesText.length;
											subtitlesText.push([text,0]);
											subtitlesText[index][1] = setTimeout(function() {
												text.remove();
												numberOfSubtitles--;
												subtitlesText[index] = null;
												if (!numberOfSubtitles) {subtitlesDiv.style.display = "none";subtitlesText = [];}
											}, (currentSubtitleS.end * 1000) - (Date.now() - whenPlayed));
											break;
										}
										case 1: {
											let text = null;
											const elements = [];
											const currentSubtitleNodes = currentSubtitleS.text;
											let classes = [];
											let prevType = null;
											for (let i = 0; i < currentSubtitleNodes.length; i++) {
												const node = currentSubtitleNodes[i];
												if (!node.text) {
													classes = node.classes;
													continue;
												}
												switch (node.type) {
													case 0: {
														text = document.createElement("a");
														break;
													}
													case 1: {
														text = document.createElement("b");
														break;
													}
													case 2: {
														text = document.createElement("i");
														break;
													}
													case 3: {
														text = document.createElement("u");
														break;
													}
													case 4: {
														text = document.createElement("s");
														break;
													}
													case 5: {
														const colorPalette = ["255,162,0","255,0,255","255,255,0","0,109,117","46,178,72","138,138,138","0,0,117","82,82,0","255,255,255","117,0,0"];
														text = document.createElement("div");
														let speaker = null;
														if (!(prevType === 5 && !(currentSubtitleNodes[i-1] instanceof VTT.VTTSubtitle))) {
															speaker = document.createElement("a");
															speaker.textContent = subtitlesToDisplay.names[node.speaker] + ": ";
															speaker.style.color = `rgb(${colorPalette[node.speaker]})`;
														}
														const dialog = document.createElement("a");
														dialog.textContent = node.text;
														if (speaker) text.appendChild(speaker);
														text.appendChild(dialog);
														text.style.backgroundColor = "black";
														break;
													}
												}
												if (node.type < 5) text.textContent = node.text;
												subtitlesDiv.appendChild(text);
												subtitlesDiv.style.display = "";
												text.style.overflowWrap = "break-word";
												text.style.wordBreak = "break-word";
												text.style.whiteSpace = "pre-wrap";
												prevType = node.type;
												if (classes.length) {
													for (const className of classes) if (className) text.classList.add(className);
												}
												elements.push(text);
											}
											const index = subtitlesText.length;
											numberOfSubtitles++;
											subtitlesText.push([elements,0]);
											subtitlesText[index][1] = setTimeout(function() {
												for (const e of elements) e.remove();
												numberOfSubtitles--;
												subtitlesText[index] = null;
												if (!numberOfSubtitles) {subtitlesDiv.style.display = "none";subtitlesText = [];}
											}, (currentSubtitleS.end * 1000) - (Date.now() - whenPlayed));
											break;
										}
									}
								}
								currentSubtitle = findSubtitleIndex((Date.now() - whenPlayed) * 0.001, currentSubtitle + 1); // skip subtitles that couldn't be displayed on time
								if (subtitleType === 0) currentSubtitleS = subtitlesToDisplay[currentSubtitle]; else if (subtitleType === 1) currentSubtitleS = subtitlesToDisplay.subtitles[currentSubtitle];
							}
						}
					}
					requestAnimationFrame(() => tick(play));
				} else {
					play.textContent = "Play Sound";
					timeline.setAttribute("fill", "none");
				}
			}
			let whenDate = 0;
			function clearSubtitles() {
				if (subtitleType === 0) {
					for (const sub of subtitlesText) {
						if (sub) {
							sub[0].remove();
							clearTimeout(sub[1]);
						}
					}
				} else if (subtitleType === 1) {
					for (const sub of subtitlesText) {
						if (sub) {
							for (const s of sub[0]) {
								s.remove();
							}
							clearTimeout(sub[1]);
						}
					}
				}
				subtitlesText = [];
				numberOfSubtitles = 0;
			}
			function stopSound() {
				if (!sharedBuffer) return true;
				if (src) {
					src.onended = function() {};
					if (playing) {
						try {
							src.stop();
						} catch {};
					}
				}
				return false;
			}
			function playSound(play) {
				if (stopSound()) return;
				src = audioContext.createBufferSource();
				src.buffer = sharedBuffer;
				src.connect(gainNode);
				startAt += timelineBeginNum / div.clientWidth * sharedBuffer.duration;
				src.start(0, startAt);
				whenPlayed = Date.now() - startAt * 1000;
				if (timeline) timeline.setAttribute("fill", "red");
				src.onended = function() {
					playing = false;
					timeline.setAttribute("fill", "none");
					clearSubtitles();
				}
				currentSubtitle = findSubtitleIndex(startAt);
				if (subtitleType === 0) {
					currentSubtitleS = subtitlesToDisplay[currentSubtitle];
				} else if (subtitleType === 1) {
					currentSubtitleS = subtitlesToDisplay.subtitles[currentSubtitle];
				}
				readyToDisplaySubtitles = true;
				clearSubtitles();
				if (!playing) requestAnimationFrame(() => tick(play));
				playing = true;
			}
			importSubtitles.onclick = async function() {
				importSubtitles.disabled = true;
				try {
					let arrayBuff = new Uint8Array(0);
					await new Promise(function(resolve, reject) {
						const file = document.createElement("input");
						file.type = "file";
						document.body.appendChild(file);
						file.oninput = function(event) {
							const firstFile = event.target.files[0];
							if (firstFile) {
								const fileReader = new FileReader();
								fileReader.onload = function(event) {
									arrayBuff = new Uint8Array(event.target.result);
									resolve();
								}
								fileReader.readAsArrayBuffer(firstFile);
							} else reject();
						}
						file.accept = ".srt, .sub, .vtt";
						file.click();
						file.remove();
					});
					let st = 0;
					if (arrayBuff[0] === 87) {st = performance.now();subtitlesToDisplay = VTT.ParseVTTFile(arrayBuff);subtitleType = 1;} else {
						const lookFor = new Uint8Array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]);
						let pointer = 0;
						while (!lookFor[arrayBuff[pointer]]) {
							pointer++;
						}
						st = performance.now();
						if (lookFor[arrayBuff[pointer]] & 0x01) {subtitlesToDisplay = SUB.ParseSUBFile(arrayBuff);} else {subtitlesToDisplay = SRT.ParseSRTFile(arrayBuff);}
						subtitleType = 0;
					}
					const en = performance.now();
					alert("Parsed the subtitles in " + ((en - st)*0.001) + " seconds.");
				} catch (err) {console.log(err.stack);}
				importSubtitles.disabled = false;
			}
			function loadDiv(exp, blank = false) {
				if (prevIntervalID !== undefined) clearInterval(prevIntervalID);
				const v = document.createElement("div");
				const x = document.createElementNS("http://www.w3.org/2000/svg", "svg");
				frame.appendChild(v);
				v.appendChild(x);
				x.setAttribute("width", exp[0].audioData.length / skipSample);
				v.style.width = (exp[0].audioData.length / skipSample) + "px";
				x.setAttribute("height", 200);
				const n = document.createElementNS("http://www.w3.org/2000/svg", "path");
				x.appendChild(n);
				n.setAttribute("fill", "white");
				n.setAttribute("stroke-width", 1);
				if (blank) data = ""; else data = generateWaveform(exp[0]);
				n.setAttribute("d", data);
				v.appendChild(document.createElement("br"));
				const play = document.createElement("button");
				play.textContent = "Play Sound";
				v.appendChild(play);
				timeline = document.createElementNS("http://www.w3.org/2000/svg", "rect");
				timeline.setAttribute("height", 200);
				timeline.setAttribute("width", 2);
				timeline.setAttribute("fill", "none");
				timeline.setAttribute("x", 0);
				timelineBegin = document.createElementNS("http://www.w3.org/2000/svg", "rect");
				timelineBegin.setAttribute("height", 200);
				timelineBegin.setAttribute("width", 2);
				timelineBegin.setAttribute("fill", "none");
				timelineBegin.setAttribute("x", 0);
				timelineEnd = document.createElementNS("http://www.w3.org/2000/svg", "rect");
				timelineEnd.setAttribute("height", 200);
				timelineEnd.setAttribute("width", 2);
				timelineEnd.setAttribute("fill", "none");
				timelineEnd.setAttribute("x", 0);
				highlight = document.createElementNS("http://www.w3.org/2000/svg", "rect");
				highlight.setAttribute("height", 200);
				highlight.setAttribute("width", 2);
				highlight.setAttribute("fill", "none");
				highlight.setAttribute("x", 0);
				x.appendChild(timeline);
				x.appendChild(timelineBegin);
				x.appendChild(timelineEnd);
				x.appendChild(highlight);
				let n2 = null;
				if (exp.length > 1 && confirm("Would you like the second channel to be displayed?")) {
					n2 = document.createElementNS("http://www.w3.org/2000/svg", "path");
					x.appendChild(n2);
					n.setAttribute("fill", "white");
					n2.setAttribute("fill", "red");
					n2.setAttribute("stroke-width", 1);
					n.setAttribute("fill-opacity", 0.5);
					n2.setAttribute("fill-opacity", 0.5);
					if (blank) dataSecond = ""; else dataSecond = generateWaveform(exp[1]);
					n2.setAttribute("d", dataSecond);
					alert("White waveform = left hearing, red waveform = right hearing. Note: If parts of the waveform are a light red, this means the audio track sounds nearly identical for both hearing sides.");
				}
				let bpmMarks = [];
				function restartSelect() {
					timelineIter = -1;
					timelineBegin.setAttribute("fill", "none");
					timelineBegin.setAttribute("x", 0);
					timelineEnd.setAttribute("fill", "none");
					timelineEnd.setAttribute("x", 0);
					highlight.setAttribute("fill", "none");
					highlight.setAttribute("x", 0);
					timelineBeginNum = 0;
					timelineEndNum = 0;
					timelineEndNumCache = 0;
					timelineBeginAudio = 0;
					timelineEndAudio = 0;
				}
				play.onclick = () => {
					if (playing) {
						stopSound();
						timeline.setAttribute("fill", "none");
						playing = false;
					} else {
						startAt = 0;
						bbb = 0;
						playSound(play);
						play.textContent = "Stop Sound";
					}
				}
				let clickedAtWhere = 0;
				const metadataOfAudio = document.createElement("pre");
				function updateMetadata() {
					let meta = "Duration: " + formatTime(exporters[0].audioData.length / exporters[0].sampleRate) + "\nSamplerate: " + exporters[0].sampleRate + "\nFilename: " + filename;
					if (clickedAtWhere) meta += "\nStarted Audio At: " + formatTime(clickedAtWhere);
					metadataOfAudio.textContent = meta;
				}
				updateMetadata();
				function playWithTimeline(eve) {
					let clickX = eve.clientX - (xOff - window.scrollX);
					if (isPressingShift) {
						timelineIter++;
						switch (timelineIter) {
							case 0: {
								timelineBegin.setAttribute("x", clickX);
								timelineBegin.setAttribute("fill", "#00AAFF");
								timelineBeginNum = clickX;
								timelineBeginAudio = timelineBeginNum * skipSample;
								break;
							}
							case 1: {
								timelineEnd.setAttribute("x", clickX);
								timelineEnd.setAttribute("fill", "#00AAFF");
								highlight.setAttribute("fill", "#00AAFF");
								highlight.setAttribute("fill-opacity", 0.3);
								timelineEndNum = clickX;
								if (timelineEndNum < timelineBeginNum) {
									const temp = timelineEndNum;
									timelineEndNum = timelineBeginNum;
									timelineBeginNum = temp;
									timelineBeginAudio = timelineBeginNum * skipSample;
								}
								timelineEndNumCache = timelineEndNum / div.clientWidth * sharedBuffer.duration;
								timelineEndAudio = timelineEndNum * skipSample;
								highlight.setAttribute("x", timelineBeginNum);
								highlight.setAttribute("width", timelineEndNum - timelineBeginNum);
								break;
							}
							case 2: {
								restartSelect();
								break;
							}
						}
						return;
					}
					const relativeX = clickX / x.clientWidth;
					startAt = relativeX * sharedBuffer.duration; // Time in seconds
					clickedAtWhere = startAt;
					updateMetadata();
					if (timelineBeginNum) startAt = 0;
					playSound(play);
					play.textContent = "Stop Sound";
				}
				x.onclick = playWithTimeline;
				selectStart.onclick = function() {
					restartSelect();
					timelineIter = 0;
					timelineBegin.setAttribute("x", 0.1/skipSample);
					timelineBegin.setAttribute("fill", "#00AAFF");
					timelineBeginNum = 0.1/skipSample;
					timelineBeginAudio = 0.1;
				}
				selectEnd.onclick = function() {
					restartSelect();
					timelineIter = 0;
					timelineBegin.setAttribute("x", exporters[0].audioData.length/skipSample);
					timelineBegin.setAttribute("fill", "#00AAFF");
					timelineBeginNum = exporters[0].audioData.length/skipSample;
					timelineBeginAudio = exporters[0].audioData.length;
				}
				const label3 = document.createElement("a");
				const co = document.createElement("input");
				label3.textContent = "View at Sample-Level";
				v.appendChild(label3);
				co.type = "checkbox";
				v.appendChild(co);
				co.oninput = function() {
					if (co.checked) {
						skipSample = 1;
						x.setAttribute("width", exp[0].audioData.length);
						v.style.width = exp[0].audioData.length + "px";
					} else {
						skipSample = 480;
						if (!data) data = generateWaveform(exp[0]);
						const width = exp[0].audioData.length / skipSample;
						x.setAttribute("width", width);
						x.style.marginLeft = "0px";
						v.style.width = width + "px";
						n.setAttribute("d", data);
						if (n2) n2.setAttribute("d", dataSecond);
						if (window.scrollX > width) window.scrollTo(width, window.scrollY);
					}
					tickCache = sharedBuffer.sampleRate / skipSample;
				}
				let count = 0;
				let yu = exp[0].audioData.length;
				window.onkeydown = function(key) {
					if (canUseSpaceKey && key.key === " ") {
						key.preventDefault();
						startAt = 0;
						bbb = 0;
						playSound(play);
					}
				}
				prevIntervalID = setInterval(function() {
					xOff = v.getBoundingClientRect().x + window.pageXOffset;
				}, 1000);
				v.appendChild(document.createElement("br"));
				const opt = document.createElement("select");
				const apply = document.createElement("button");
				const applyToSpecific = document.createElement("button");
				const params = document.createElement("div");
				const bpm = document.createElement("button");
				v.appendChild(bpm);
				bpm.textContent = "Label BPM";
				bpm.onclick = function() {
					if (bpmMarks.length > 0) {
						for (const b of bpmMarks) b.remove();
					}
					bpmMarks = [];
					const bpmNum = Number(prompt("What is the amount of beats per minute of this track?") || NaN);
					if (!isNaN(bpmNum)) {
						if (bpmNum > 2400) {
							alert("There are too many BPM.");
							return;
						} else if (bpmNum <= 1) {
							alert("The amount of BPM is too low.");
							return;
						}
						const inc = (60 / bpmNum) * (div.clientWidth / sharedBuffer.duration);
						const data1 = document.createElementNS("http://www.w3.org/2000/svg", "path");
						data1.setAttribute("stroke", "yellow");
						data1.setAttribute("fill", "none");
						data1.setAttribute("width", div.clientWidth);
						data1.setAttribute("height", "200");
						const data2 = document.createElementNS("http://www.w3.org/2000/svg", "path");
						data2.setAttribute("stroke", "orange");
						data1.setAttribute("fill", "none");
						data2.setAttribute("width", div.clientWidth);
						data1.setAttribute("height", "200");
						let data1Data = [], data2Data = [];
						const wid = div.clientWidth;
						for (let i = 0, j = 0; i < wid; i += inc, j++) {
							if (j & 3) data2Data.push(`M${i},0 V200`); else data1Data.push(`M${i},0 V200`);
						}
						data1.setAttribute("d", data1Data.join(" "));
						data2.setAttribute("d", data2Data.join(" "));
						x.appendChild(data1);
						x.appendChild(data2);
						bpmMarks.push(data1, data2);
					}
				}
				v.appendChild(document.createElement("br"));
				const renameFile = document.createElement("button");
				v.appendChild(renameFile);
				renameFile.textContent = "Rename Audio";
				renameFile.onclick = function() {
					filename = prompt("What will be the name of the edited audio?");
					updateMetadata();
				}
				v.appendChild(document.createElement("br"));
				v.appendChild(metadataOfAudio);
				v.appendChild(document.createElement("br"));
				{
					const label = document.createElement("a");
					label.textContent = "Effect To Apply: ";
					let option = document.createElement("option");
					option.textContent = "Please select an effect";
					option.value = "placeholder";
					v.appendChild(label);
					v.appendChild(opt);
					opt.appendChild(option);
					for (let effect of effectsList) {
						option = document.createElement("option");
						option.textContent = effect[0];
						option.value = effect[4];
						opt.appendChild(option);
						if (effect[6] && effect[6] > 1) option.style.color = "gray";
					}
					v.appendChild(params);
					apply.textContent = "Apply";
					v.appendChild(apply);
					applyToSpecific.textContent = "Apply To Specific Channel";
					v.appendChild(applyToSpecific);
					let currentSelected = null, paramElements = null, effect = [];
					async function applyEffectOnExporter(i) {
						const expor = exporters[i];
						const values = [expor, ...paramElements.map((x, i) => {
							if (x.type === "checkbox") {
								return effect[5][i](x.checked);
							} else {
								return effect[5][i](x.value);
							}
						})];
						if (effect[6] > 1) values[0] = exporters;
						if (timelineBeginAudio !== 0 && effect[4] !== "trim") {
							if (values[0] === exporters) {
								values[0] = [{audioData: exporters[0].audioData.subarray(Math.floor(timelineBeginAudio), timelineEndAudio), sampleRate: exporters[0].sampleRate}, {audioData: exporters[1].audioData.subarray(Math.floor(timelineBeginAudio), timelineEndAudio), sampleRate: exporters[1].sampleRate}];
							} else {
								values[0] = {audioData: expor.audioData.subarray(Math.floor(timelineBeginAudio), timelineEndAudio), sampleRate: expor.sampleRate};
							}
						}
						const le = (effect[6] > 1 ? values[0][0] : values[0]).audioData.length;
						const exporterT = (effect[6] > 1 ? values[0][i] : values[0]);
						await effects[effect[4]](...values);
						if (timelineBeginAudio !== 0) {
							if (exporterT.audioData.buffer !== expor.audioData.buffer) {
								if (exporterT.audioData.length === Math.floor(timelineBeginAudio) - timelineBeginAudio) {
									expor.audioData.set(exporterT.audioData, Math.floor(timelineBeginAudio));
								} else {
									const data = new Float32Array(exporterT.audioData.length + expor.audioData.length - le);
									data.set(expor.audioData.subarray(0, Math.floor(timelineBeginAudio)), 0);
									data.set(exporterT.audioData, Math.floor(timelineBeginAudio));
									data.set(expor.audioData.subarray(timelineEndAudio, expor.audioData.length), exporterT.audioData.length + Math.floor(timelineBeginAudio));
									expor.audioData = data;
									//timelineEndAudio = values[0].audioData.length - timelineBeginAudio;
								}
							}
						}
						return le;
					}
					async function applyEffect() {
						if (currentSelected) {
							apply.disabled = true;
							const lens = new Array(exporters.length);
							for (let i = 0; i < exporters.length; i++) {
								lens[i] = await applyEffectOnExporter(i);
							}
							if (exporters[1] && exporters[1].audioData.length !== exporters[0].audioData.length) {
								const index = (!exporters[1] || exporters[0].audioData.length > exporters[1].audioData.length) ? 0 : 1;
								const indexInverse = 1 - index;
								effects["blank"](exporters[indexInverse], (exporters[index].audioData.length - exporters[indexInverse].audioData.length) / exporters[index].sampleRate, confirm("It looks like the channels have unique lengths, which would cause unexpected bugs during exporting. Do you want to pad silence at the beginning (Yes/OK) or end (Cancel/No)?") ? "f" : "");
							}
							data = generateWaveform(exporters[0]);
							if (n2 && exporters[1]) dataSecond = generateWaveform(exporters[1]);
							if (exporters[0].audioData.length !== lens[0] || (exporters[1] && exporters[1].audioData.length === lens[1])) sharedBuffer = audioContext.createBuffer(exporters.length, exporters[0].audioData.length, exporters[0].sampleRate);
							const ref = sharedBuffer.getChannelData(0);
							ref.set(exporters[0].audioData);
							if (exporters.length >= 2) {
								const ref2 = sharedBuffer.getChannelData(1);
								ref2.set(exporters[1].audioData);
							} else {
								if (sharedBuffer.numberOfChannels > 1) {
									const ref2 = sharedBuffer.getChannelData(1);
									ref2.set(exporters[0].audioData);
								}
							}
							x.setAttribute("width", exp[0].audioData.length / skipSample);
							v.style.width = (exp[0].audioData.length / skipSample) + "px";
							n.setAttribute("d", data);
							if (n2) n2.setAttribute("d", dataSecond);
							apply.disabled = false;
							x.onclick = playWithTimeline; // Reset playing function, just in case browsers like Chrome misbehave every time an effect is applied
							yu = exp[0].audioData.length;
							updateMetadata();
						}
					}
					apply.onclick = async function() {await applyEffect();}
					applyToSpecific.onclick = async function() {
						const numberIndexes = ["first", "second", "third", "fourth"];
						await applyEffectOnExporter((prompt("Which channel do you want to apply the effect to? (" + (exporters.map((x, i) => {return (i+1) + " = " + numberIndexes[i] + " channel";}).join(", ")) + ")") ?? 1) - 1);
					}
					let previous = 0;
					opt.oninput = function() {
						effect = effectsList.find(x => x[4] === opt.value);
						if (effect[6] && sharedBuffer.numberOfChannels < effect[6]) {
							const d = ["mono", "stereo"];
							alert("That effect doesn't support " + d[exporters.length - 1] + " audio! Tip: Gray effects mean that they support " + d[effect[6] - 1] + " audio.");
							opt.selectedIndex = previous;
							return;
						}
						currentSelected = effect;
						if (effect) {
							params.innerHTML = effect[1] + "<br>" + effect[2];
							paramElements = new Array(effect[3]);
							for (let i = 0; i < paramElements.length; i++) paramElements[i] = document.getElementById(effect[4] + i);
						} else {params.innerHTML = "";paramElements = null;}
						params.style.width = "100vw";
						previous = opt.selectedIndex;
					}
				}
				tickCache = sharedBuffer.sampleRate / skipSample;
				v.appendChild(document.createElement("br"));
				let exportBtn = document.createElement("button");
				v.appendChild(exportBtn);
				exportBtn.textContent = "Export as WAV (Original Library)";
				exportBtn.onclick = function() {
					const metadata = {};
					const author = prompt("Who is the author of this audio? (e.g. \"John Smith\") Leave the input blank if you don't want the author to be specified.");
					if (author === "" || author === undefined || author === null) return;
					const name = prompt("What is the name of this audio? (e.g. \"Electro Dance (Techno Mix)\") Leave the input blank if you don't want the name to be specified.");
					const genre = prompt("What is the genre of this audio? (e.g. \"Techno\") Leave the input blank if you don't want the genre to be specified.");
					const notes = prompt("Any notes or comments for this audio? (e.g. \"This audio is super cool\") Leave the input blank if you don't have any notes or comments.");
					const software = prompt("What software did you use to create this audio? (e.g. \"UnnamedBruh's Audio Editor\") If input is \"UAE\" without quotes, this software's name will be appended to the audio metadata.");
					if (author) metadata["IART"] = author;
					if (name) metadata["INAM"] = name;
					if (genre) metadata["IGNR"] = genre;
					if (notes) metadata["ICMT"] = notes;
					if (software) metadata["ISFT"] = software === "UAE" ? "UnnamedBruh's Audio Editor" : software;
					const encodeBits = prompt("What are the exporting settings of the audio? (0 = 8-bit. 1 = 12-bit. 2 = 16-bit. 3 = 24-bit. 4 = 32-bit. 5 = 32-bit float. 6 = 64-bit float. 7 = mu-law. 8 = a-law. 9 = dpcm.)");
					const exporters2 = timelineBeginAudio ? {audioData: exporters[0].audioData.subarray(timelineBeginAudio, timelineEndAudio), sampleRate: exporters[0].sampleRate, bits: exporters[0].bits, encoding: exporters[0].encoding, convertToWav: exporters[0].convertToWav} : exporters[0];
					const oldBits = exporters2.bits;
					let isCompatible = false;
					if (encodeBits == 0) {
						exporters2.bits = 8;
						exporters2.encoding = "pcm";
					} else if (encodeBits == 1) {
						exporters2.bits = 12;
						exporters2.encoding = "pcm";
						isCompatible = confirm("Older commercial software used to export 12-bit WAV audio using this ratio: 3:2 (3 bytes per 2 samples). However, modern audio decoders expect byte-aligned 12-bit WAV audio that use 4:2 (4 bytes per 2 samples).\n\nDo you want byte-aligned audio?");
					} else if (!encodeBits || encodeBits == 2) {
						exporters2.bits = 16;
						exporters2.encoding = "pcm";
					} else if (encodeBits == 3) {
						exporters2.bits = 24;
						exporters2.encoding = "pcm";
					} else if (encodeBits == 4) {
						exporters2.bits = 32;
						exporters2.encoding = "pcm";
					} else if (encodeBits == 5) {
						exporters2.bits = 32;
						exporters2.encoding = "pcmf32";
					} else if (encodeBits == 6) {
						exporters2.bits = 64;
						exporters2.encoding = "pcmf64";
					} else if (encodeBits == 7) {
						exporters2.bits = 8;
						exporters2.encoding = "mulaw";
					} else if (encodeBits == 8) {
						exporters2.bits = 8;
						exporters2.encoding = "alaw";
					} else if (encodeBits == 9) {
						const whichFormat = prompt("Which DPCM audio format do you need for playback? Note: DPCM encoding isn't fully standardized. For example, one system (think NES) assumes a certain format, but another (think Windows) may assume a different one.\n\n0 = NES hardware, 1-bit differences, 4-bit samples\n1 = Xan DPCM, for DOS games by Origin Systems");
						if (whichFormat == 0) {
							exporters2.bits = 1;
							exporters2.encoding = "nesdpcm";
						} else if (whichFormat == 1) {
							exporters2.bits = 8;
							exporters2.encoding = "xandpcm";
						}
					}
					const blob = exporters2.convertToWav(metadata, exporters.length > 1 ? (timelineBeginAudio ? exporters[0].audioData.subarray(timelineBeginAudio, timelineEndAudio) : exporters[1].audioData) : undefined, isCompatible);
					exporters2.bits = encodeBits;
					downloadFile(blob, ".wav");
				}
				const exportBtn1 = document.createElement("button");
				v.appendChild(exportBtn1);
				exportBtn1.textContent = "Sierra Audio (.SOL or .AUD)";
				exportBtn1.onclick = async function() {
					const encodeBits = prompt("What are the exporting settings of the audio? (0 = 8-bit. 1 = 16-bit. 2 = 8-bit old ADPCM. 3 = 8-bit new ADPCM. 4 = 16-bit new ADPCM.)");
					const exporters2 = timelineBeginAudio ? {audioData: exporters[0].audioData.subarray(timelineBeginAudio, timelineEndAudio), sampleRate: exporters[0].sampleRate, bits: exporters[0].bits, encoding: exporters[0].encoding, convertToWav: exporters[0].convertToWav} : exporters[0];
					const oldBits = exporters2.bits;
					if (encodeBits == 0) {
						exporters2.bits = 8;
						exporters2.encoding = "pcm";
					} else if (!encodeBits || encodeBits == 1) {
						exporters2.bits = 16;
						exporters2.encoding = "pcm";
					} else if (encodeBits == 2) {
						exporters2.bits = 8;
						exporters2.encoding = "dpcmold";
					} else if (encodeBits == 3) {
						exporters2.bits = 8;
						exporters2.encoding = "dpcm";
					} else if (encodeBits == 4) {
						exporters2.bits = 16;
						exporters2.encoding = "dpcm";
					}
					const blob = await exporters2.convertToSol(exporters.length > 1 ? exporters[1].audioData : undefined);
					exporters2.bits = encodeBits;
					downloadFile(blob, ".sol");
				}
				const exportBtn2 = document.createElement("button");
				v.appendChild(exportBtn2);
				exportBtn2.textContent = "Export as MP3 (liblamejs)";
				exportBtn2.onclick = async function() {
					function floatTo16BitPCM(float32Array, channels) {
						const len = float32Array.length / channels;
						const buffer = new Int16Array(len);
						let s = 0;
						if (channels === 1) {
							for (let i = 0; i < len; i++) {
								s = float32Array[i];
								buffer[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
							}
						} else {
							for (let i = 0, j = 0; i < len; i++, j+=channels) {
								s = float32Array[j|0];
								buffer[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
							}
						}
						return buffer;
					}
					let sampleRate = exporters[0].sampleRate;
					const kbps = Math.ceil(Number(prompt("What is the KBPS (Kilobits Per Second) of the MP3 audio? (e.g. 8, 16, 32, 64, 80, 96, 128, 168, 192, 256, 320)")) / 8) * 8;
					if (kbps === 0 || isNaN(kbps) || kbps === undefined || kbps === null) return;
					const qual = Math.min(9, Math.max(0, prompt("What is the VBR (Variable BitRate) of the MP3 audio? (between 0 and 9. 0 = best quality and slower encoding, 9 = worst quality and faster encoding, 3 = balanced)") ?? 3));
					const channels = (exporters.length === 2) ? 2 : 1;
					const shouldSplit = channels !== 2 && confirm("Should the same buffer be split into two channels? (If so, the audio may sound 2X lower-quality)");
					const isJoint = (channels === 2 || shouldSplit) ? prompt("Should the encoder use Mid/Side analysis to decide the optimal # of channels? (0 = No, 1 = Yes, 2 = Yes; encode the audio into Mid/Side to catch LAME off-guard)") : 0;
					const lossTimes = Number(prompt("How many times should the same audio be re-encoded? (for glitch-art audio)") || 1);
					let blob;
					exportBtn2.disabled = true;
					const fixEncodeTable = {32: 0.75, 16: 0.75, 8: 0.375, 168: 1.312637701347876, 256: 2, 320: 2.495};
					const fixEncodeTable2 = {32: 2.177427725956358, 16: 3.0012627195702253, 128: 1.0891036274839623};
					const fixEncodeTable3 = {128: 1.088449674954518};
					const tttt = fixEncodeTable[kbps] || 1;
					for (let i = 0; i < lossTimes; i++) {
						let buffer = i === 0 ? exporters.map(exp => timelineBeginAudio ? exp.audioData.subarray(timelineBeginAudio, timelineEndAudio) : exp.audioData) : await audioContext.decodeAudioData(await blob.arrayBuffer());
						let altRate = buffer.sampleRate;
						if (i > 0) {
							if (buffer.numberOfChannels === 2) buffer = [buffer.getChannelData(0), buffer.getChannelData(1)]; else buffer = [buffer.getChannelData(0), buffer.getChannelData(0)];
						}
						const mp3Encoder = new lamejs.Mp3Encoder(channels + shouldSplit, i === 0 ? sampleRate : altRate, kbps, qual, !!isJoint);
						let mp3Data = [];
						let quantity = channels === 2 || shouldSplit ? ((2 / (kbps/64)) * tttt) : (fixEncodeTable2[kbps] || 1);
						if (isJoint && isJoint != 0) quantity *= fixEncodeTable3[kbps] || 1;
						let pcmData = [floatTo16BitPCM(buffer[0], quantity)];
						const len = pcmData[0].length;
						if (channels === 1) pcmData = [pcmData[0], pcmData[0]]; else pcmData = [pcmData[0], floatTo16BitPCM(buffer[1], quantity)];
						if (isJoint && isJoint != 0) {
							if (isJoint == 2) {
								let temp = 0, len = pcmData[0].length;
								const left = pcmData[0], right = pcmData[1];
								for (let i = 0; i < len; i++) {
									temp = left[i];
									left[i] = (left[i] + right[i]) * 0.5;
									right[i] = (temp - right[i]) * 0.5;
								}
							}
						}
						const len2 = len - 1152;
						let xx;
						for (let i = 0; i < len2; i += 1152) {
							xx = i > len2 ? len : i + 1152;
							const chunk = [pcmData[0].subarray(i, xx), pcmData[1].subarray(i, xx)];
							// For mono input, pass chunk for both channels if needed
							const mp3Chunk = mp3Encoder.encodeBuffer(chunk[0], chunk[1]);
							mp3Data.push(mp3Chunk);
						}
						mp3Data.push(mp3Encoder.flush());
						blob = new Blob(mp3Data, { type: "audio/mp3" });
					}
					exportBtn2.disabled = false;
					downloadFile(blob, ".mp3");
				}
				v.appendChild(document.createElement("br"));
				const exportBtn3 = document.createElement("button");
				v.appendChild(exportBtn3);
				exportBtn3.textContent = "Export as OGG (Vorbis, wasm-media-encoders)";
				exportBtn3.onclick = async function() {
					exportBtn3.disabled = true;
					try {
						const vbr = Number(prompt("How high can the quality of the audio be? (-1.0 = worst quality, low sizes; 10.0 = best quality, higher sizes; 4.0 = balanced quality, minimal differences, medium sizes; -2.0 = absolute worst, lowest sizes)") ?? 4.0) ?? 4.0;
						let blob;
						const split = exporters.length !== 2 && !!confirm("Should the audio be split into two separate channels?");
						const times = Math.ceil(Math.abs(Number(prompt("How many times would the audio be re-encoded?") || 1) || 1));
						for (let i = 0; i < times; i++) {
							const encoder = await WasmMediaEncoder.createOggEncoder();
							let buffer = i === 0 ? exporters.map(exp => timelineBeginAudio ? exp.audioData.subarray(timelineBeginAudio, timelineEndAudio) : exp.audioData) : await audioContext.decodeAudioData(await blob.arrayBuffer());
							let altRate = buffer.sampleRate;
							if (i > 0) {
								if (buffer.numberOfChannels === 2) buffer = [buffer.getChannelData(0), buffer.getChannelData(1)]; else buffer = [buffer.getChannelData(0)];
							}
							encoder.configure({
								sampleRate: i === 0 ? exporters[0].sampleRate : altRate,
								channels: Math.min(2, buffer.length + split),
								vbrQuality: vbr,
							});
							const encodedData = encoder.encode(split || buffer.length === 2 ? [buffer[0], buffer[1] || buffer[0]] : [buffer[0]]);
							const encodedCopy = new Uint8Array(encodedData).buffer;
							const finalData = encoder.finalize();
							blob = new Blob([encodedCopy, finalData], { type: "audio/ogg" });
						}
						downloadFile(blob, ".ogg");
					} catch (err) {
						alert("It seems like the OGG Vorbis codec is supported, but not yet available. A network is required to export your audio into the OGG format.");
						console.log(err.message, err.stack);
					}
					exportBtn3.disabled = false;
				}
				const exportBtn4 = document.createElement("button");
				v.appendChild(exportBtn4);
				exportBtn4.textContent = "Export as OGG (Opus, libopusjs)";
				exportBtn4.onclick = async function() {
					exportBtn4.disabled = true;
					function floatTo16BitPCM(float32Array, channels) {
						const len = float32Array[0].length;
						const buffer = new Int16Array(len * channels);
						let s = 0;
						if (channels === 1) {
							for (let i = 0; i < len; i++) {
								s = float32Array[0][i];
								buffer[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
							}
						} else {
							for (let i = 0, j = 0; i < len; i++, j+=channels) {
								s = float32Array[0][i];
								buffer[j] = s < 0 ? s * 0x8000 : s * 0x7fff;
								s = float32Array[1][i];
								buffer[j + 1] = s < 0 ? s * 0x8000 : s * 0x7fff;
							}
						}
						return buffer;
					}
// Simple Ogg-Opus muxer (browser compatible) (UNNAMEDBRUH DID NOT WRITE ANY OF THE OGG MUXING LOGIC!!! GPT-5.0 Non-mini/Mini, and Claude Sonnet 4.5 DID THIS!)
// - Expects: packets = Array<Uint8Array> (raw Opus packets, in order)
// - Options:
//		sampleRate: number (default 48000)
//		channels: number (default 2)
//		serial: 32-bit stream serial (random by default)
//		pageSizeLimit: approx max body size per page (defaults chosen for decent packing)
//		getPacketSamples(packet): function(Uint8Array) -> number (samples per channel)
//			 If not provided, default assumes frames of 20ms (960 samples @48kHz) and
//			 parses simple TOC frame-count rules (good if your encoder uses 20ms frames).
// Returns: Blob (type 'audio/ogg')
// References:
//	- Ogg framing: RFC 3533 / xiph.org docs.
//	- Opus in Ogg: RFC 7845.
//	- Opus packet TOC: RFC 6716 (used for a basic frame-count calculation).
//
// This is intentionally minimal; production muxers do additional checks and
// better packing heuristics.
// Full browser-ready Ogg-Opus muxer (minimal, RFC-compliant) — corrected paging logic
function muxOpusPacketsToOgg(packets, opts = {}) {
	const {
		sampleRate = 48000,
		channels = 2,
		serial = (Math.random() * 0xffffffff) >>> 0,
		pageSizeLimit = 65000,
		getPacketSamples = defaultGetPacketSamples(20, sampleRate),
		vendor = "libopusjs-muxer",
		debug = true
	} = opts;
	// --- Helper functions (unchanged) ---
	const concatUint8 = (chunks) => {
		const total = chunks.reduce((sum, c) => sum + c.length, 0);
		const out = new Uint8Array(total);
		let offset = 0;
		for (const c of chunks) { out.set(c, offset); offset += c.length; }
		return out;
	};
	function write32LE(buf, offset, value) {
		const dv = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
		dv.setUint32(offset, value >>> 0, true);
	}
	function write64LE(buf, offset, value) {
		const dv = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
		const v = BigInt(value);
		dv.setUint32(offset, Number(v & 0xffffffffn), true);
		dv.setUint32(offset + 4, Number((v >> 32n) & 0xffffffffn), true);
	}
	// CRC32 table (unchanged)
	const CRC32 = (() => {
		const table = new Uint32Array(256);
		for (let i = 0; i < 256; i++) {
			let r = i;
			for (let j = 0; j < 8; j++) {
				r = (r & 1) ? (0xEDB88320 ^ (r >>> 1)) : (r >>> 1);
			}
			table[i] = r >>> 0;
		}
		return (buf) => {
			let crc = 0xffffffff;
			for (let i = 0; i < buf.length; i++) crc = (crc >>> 8) ^ table[(crc ^ buf[i]) & 0xff];
			return (crc ^ 0xffffffff) >>> 0;
		};
	})();
	// OpusHead (unchanged)
	function createOpusHead() {
		const buf = new Uint8Array(19);
		buf.set(new TextEncoder().encode("OpusHead"), 0);
		buf[8] = 1;			  // version
		buf[9] = channels;	   // channel count
		buf[10] = 0x38; buf[11] = 0x01; // pre-skip 312 samples
		buf[12] = sampleRate & 0xff; buf[13] = (sampleRate >> 8) & 0xff;
		buf[14] = (sampleRate >> 16) & 0xff; buf[15] = (sampleRate >> 24) & 0xff;
		buf[16] = buf[17] = 0;   // gain
		buf[18] = 0;			 // mapping family
		return buf;
	}
	// OpusTags (unchanged)
	function createOpusTags(vendorStr = vendor, comments = {}) {
		const vendorBytes = new TextEncoder().encode(vendorStr);
		const userComments = Object.entries(comments).map(([k,v]) => `${k}=${v}`);
		const commentBytes = userComments.map(s => new TextEncoder().encode(s));
		let totalLen = 8 + 4 + vendorBytes.length + 4;
		for (const b of commentBytes) totalLen += 4 + b.length;
		const buf = new Uint8Array(totalLen);
		const dv = new DataView(buf.buffer);
		let offset = 0;
		buf.set(new TextEncoder().encode("OpusTags"), offset); offset += 8;
		dv.setUint32(offset, vendorBytes.length, true); offset += 4;
		buf.set(vendorBytes, offset); offset += vendorBytes.length;
		dv.setUint32(offset, commentBytes.length, true); offset += 4;
		for (const b of commentBytes) { dv.setUint32(offset, b.length, true); offset += 4; buf.set(b, offset); offset += b.length; }
		return buf;
	}
	// --- New robust page builder ---
	const pages = [];
	let pageSeq = 0;
	let granule = 0n; // absolute samples (per-channel) up to last completed packet
	const MAX_SEGMENTS = 255;
	function buildAndPushPage(segmentTable, bodyParts, pageStartedWithContinuation, isBOS, isEOS) {
		const header = new Uint8Array(27 + segmentTable.length);
		header.set(new TextEncoder().encode("OggS"), 0);
		header[4] = 0; // version
		let flags = 0x00;
		if (pageStartedWithContinuation) flags |= 0x01;
		if (isBOS) flags |= 0x02;
		if (isEOS) flags |= 0x04;
		header[5] = flags;
		// granule must be set before CRC calculation; pass the current granule (already updated for completed packets)
		write64LE(header, 6, granule);
		write32LE(header, 14, serial);
		write32LE(header, 18, pageSeq++);
		write32LE(header, 22, 0); // CRC placeholder
		header[26] = segmentTable.length;
		header.set(segmentTable, 27);
		const pageBody = concatUint8(bodyParts);
		// compute CRC over header(with CRC=0) + body, then write CRC into header and push
		const fullPage = concatUint8([header, pageBody]);
		const crc = CRC32(fullPage);
		write32LE(header, 22, crc);
		pages.push(concatUint8([header, pageBody]));
	}
	// --- Write BOS pages (OpusHead + OpusTags) ---
	buildAndPushPage(
		new Uint8Array([createOpusHead().length]), // temporary single-segment trick doesn't matter here because we put OpusHead as a single packet below
		[createOpusHead()],
		false, /*continued*/ true, /*BOS*/ false /*EOS*/
	);
	buildAndPushPage(
		(function(){ // OpusTags may be larger than 255 segments but almost never; treat it as one full packet normally
			const t = createOpusTags(vendor, {});
			const segs = [];
			let off = 0;
			while (off < t.length) { const take = Math.min(255, t.length - off); segs.push(take); off += take; }
			return new Uint8Array(segs);
		})(),
		[createOpusTags(vendor, {})],
		true, /*continued*/ false /*BOS*/ , false /*EOS*/
	);
	// --- Audio packet to pages logic (robust) ---
	let pktIndex = 0;
	let pktOffset = 0; // byte offset inside current packet (for continuation)
	while (pktIndex < packets.length) {
		// start a new page
		const segs = [];
		const bodyParts = [];
		let bodySize = 0;
		const pageStartedWithContinuation = pktOffset !== 0;
		let segmentsCount = 0;
		// We'll remember whether any packet *completed* on this page so we can update granule accordingly.
		// granule already contains samples up to the last completed packet before this page.
		// When a packet completes on this page we add its samples to granule immediately so final granule reflects last-complete.
		let anyCompletionOnThisPage = false;
		while (segmentsCount < MAX_SEGMENTS && bodySize < pageSizeLimit && pktIndex < packets.length) {
			const pkt = packets[pktIndex];
			const remaining = pkt.length - pktOffset;
			const toTake = Math.min(255, remaining);
			// If adding this segment would exceed the pageSizeLimit, break.
			if (bodySize + toTake > pageSizeLimit) {
				// If bodySize is zero and toTake > pageSizeLimit, we still must put at least one segment (even if > limit) — but toTake <= 255 so okay.
				break;
			}
			// append segment and body chunk
			segs.push(toTake);
			bodyParts.push(pkt.subarray(pktOffset, pktOffset + toTake));
			bodySize += toTake;
			segmentsCount += 1;
			pktOffset += toTake;
			if (pktOffset === pkt.length) {
				// packet completed on this page — increment granule by its sample count
				const samples = BigInt(getPacketSamples(pkt));
				granule += samples;
				anyCompletionOnThisPage = true;
				// move to next packet
				pktIndex += 1;
				pktOffset = 0;
			} else {
				// packet still continues to next page
				// we must stop if segmentsCount reached 255 (outer loop checks), otherwise the outer while will continue
				// but we must break if we've filled the page (bodySize >= pageSizeLimit) — outer loop checks
			}
		}
		// Decide EOS: only true for the final page after we've processed all packets (and no remaining partial)
		const isEOS = (pktIndex >= packets.length && pktOffset === 0);
		// Special rule: if this page contains NO completed packets, granule must be set to the previous granule value (which it already is)
		// build segment table Uint8Array
		const segmentTable = new Uint8Array(segs);
		// push page
		buildAndPushPage(segmentTable, bodyParts, pageStartedWithContinuation, false, isEOS);
		// Continue; the loop will start next page with pktOffset possibly non-zero (continuation)
	}
	if (debug) console.log(`Created ${pages.length} pages, final granule: ${granule}`);
	return new Blob(pages, { type: "audio/ogg" });
}
// --- Default packet sample counter ---
function defaultGetPacketSamples(frameMs = 20, sampleRate = 48000) {
	const frameSamples = Math.round(sampleRate * (frameMs / 1000));
	return function(packet) {
		if (!packet || packet.length === 0) return 0;
		const toc = packet[0];
		const code = toc & 0x03;
		let frames = 1;
		if (code === 1 || code === 2) frames = 2;
		else if (code === 3 && packet.length >= 2) frames = packet[1] & 0x3f || 1;
		return frames * frameSamples;
	};
}
// Granule calculating is correct, but issues might be packet or CRT calculation
					try {
						const vbr = Number(prompt("How high can the bitrate of the audio be? (6.0 = worst quality, big differences, low sizes; 512.0 = best quality, nearly-neglible differences, higher sizes; 256.0 = balanced quality, minimal differences, medium sizes)") ?? 256.0) ?? 256.0;
						const voiceOpt = confirm("Should the audio encoder optimize for voices in the audio?");
						let blob;
						const split = exporters.length !== 2 && !!confirm("Should the audio be split into two separate channels?");
						const times = Number(prompt("How many times would the audio be re-encoded?") || 1) || 1;
						for (let i = 0; i < times; i++) {
							let buffer = i === 0 ? exporters.map(exp => timelineBeginAudio ? exp.audioData.subarray(timelineBeginAudio, timelineEndAudio) : exp.audioData) : await audioContext.decodeAudioData(await blob.arrayBuffer());
							let altRate = buffer.sampleRate;
							if (i > 0) {
								if (buffer.numberOfChannels === 2) buffer = [buffer.getChannelData(0), buffer.getChannelData(1)]; else buffer = [buffer.getChannelData(0)];
							}
// Rest of encoding logic is implemented by GPT-5.0 Mini.
		// Prepare channels
		const bufferF = split || buffer.length === 2 ? [buffer[0], buffer[1] || buffer[0]] : [buffer[0]];
		const channels = bufferF.length;
							const sampleRate = altRate || exporters[0].sampleRate; // Except this line, I added this.
		const encoder = new Encoder(channels, sampleRate, vbr, 20, voiceOpt);
		const arr = [];
		const frameSize = sampleRate * 20 / 1000; // 20ms @ 48kHz | Except this line, this altered modified by me.
		const totalSamples = bufferF[0].length;
		for (let pos = 0; pos + frameSize <= totalSamples; pos += frameSize) {
			const frame = bufferF.map(ch => ch.subarray(pos, pos + frameSize));
			const pcm = floatTo16BitPCM(frame, channels);
			encoder.input(pcm);
			let pkt;
			while ((pkt = encoder.output()) != null) {
									arr.push(pkt);
								}
							}
							blob = muxOpusPacketsToOgg(arr, {
								sampleRate,
								channels
							});
							encoder.destroy();
						}
						downloadFile(blob, ".ogg");
					} catch (err) {
						alert("It seems like the OGG Opus codec is supported, but not yet available. READ BELOW:\n\nA network is required to export your audio into the OGG format. The encoder itself is stored in an online module server (UNPKG.com), then loaded into this audio editor. (This means your audio track or other invaluable information isn't tracked, or sent to any server; just encoded directly in your browser!)");
						console.log(err.message, err.stack);
					}
					exportBtn4.disabled = false;
				}
				const exportBtn5 = document.createElement("button");
				v.appendChild(exportBtn5);
				exportBtn5.textContent = "Export as FLAC (libflac.js)";
				exportBtn5.onclick = async function() { // This function was written entirely by Claude Sonnet 4.5. I made some modifications to the original code.
					exportBtn5.disabled = true;
					try {
						const encBuffer = [];
						function write_callback_fn(buffer) {
							const copy = new Uint8Array(buffer);
							encBuffer.push(copy);
						}
						const CHANNELS = exporters.length;
						const SAMPLERATE = exporters[0].sampleRate;
						const BPS = Number(prompt("What is the bits per sample of the FLAC audio? (Although 8-bit is nonstandard, it is supported across most systems.)") ?? 24)>>3<<3; // 24-, 16-, or 8-bit encoding
						const COMPRESSION = Number(prompt("What is the quality of the audio? (In other terms, how aggressive is the lossless compression? 0 = fastest compression, largest file, 8 = slower compression, smaller file, 5 = balanced compression, balanced file)") ?? 5);
						const VERIFY = false;
						const BLOCK_SIZE = 0; // 0 = let encoder decide
						const junkMode = (BPS !== 16 || confirm("Do you want your FLAC to not have any garbage data?")) ? 1 : 0;
						const flac_encoder = Flac.create_libflac_encoder(
							SAMPLERATE,
							CHANNELS,
							BPS,
							COMPRESSION,
							(timelineBeginAudio ? (timelineEndAudio - timelineBeginAudio) : exporters[0].audioData.length) * (2 - junkMode), // total_samples (0 = unknown)
							VERIFY,
							BLOCK_SIZE
						);
						const initStatus = Flac.init_encoder_stream(
							flac_encoder,
							write_callback_fn
						);
						if (initStatus !== 0) {
							throw new Error("FLAC encoder initialization failed");
						}
						const audioData = exporters.map(i => timelineBeginAudio ? i.audioData.subarray(timelineBeginAudio, timelineEndAudio) : i.audioData);
						let encodedStatus = false;
						if (BPS === 24) {
							const hh = audioData[0].length * audioData.length;
							const int24Buffer = new Int32Array(Math.ceil(hh / 4096) * 4096); // FLAC encoders often expect 4096 for block sizes.
							if (audioData.length === 2) {
								const aa = audioData[0];
								const ab = audioData[1];
								for (let i = 0, j = 0; i < hh; i += 2, j++) {
									int24Buffer[i] = (aa[j] * 8388607) | 0;
									int24Buffer[i + 1] = (ab[j] * 8388607) | 0;
								}
							} else {
								const aa = audioData[0];
								for (let i = 0; i < aa.length; i++) {
									int24Buffer[i] = (aa[i] * 8388607) | 0;
								}
							}
							encodedStatus = Flac.FLAC__stream_encoder_process_interleaved(
								flac_encoder,
								int24Buffer,
								audioData[0].length
							);
						} else if (BPS === 8) {
							const hh = audioData[0].length * audioData.length;
							const int8Buffer = new Int32Array(Math.ceil(hh / 4096) * 4096); // FLAC encoders often expect 4096 for block sizes.
							if (audioData.length === 2) {
								const aa = audioData[0];
								const ab = audioData[1];
								for (let i = 0, j = 0; i < hh; i += 2, j++) {
									int8Buffer[i] = (aa[j] * 127) | 0;
									int8Buffer[i + 1] = (ab[j] * 127) | 0;
								}
							} else {
								const aa = audioData[0];
								for (let i = 0; i < aa.length; i++) {
									int8Buffer[i] = (aa[i] * 127) | 0;
								}
							}
							encodedStatus = Flac.FLAC__stream_encoder_process_interleaved(
								flac_encoder,
								int8Buffer,
								audioData[0].length
							);
						} else if (BPS === 16) {
							let int16Buffer = new Int16Array();
							const len = audioData[0].length * audioData.length * 2;
							if (!junkMode) int16Buffer = new Int16Array(len * 2); else int16Buffer = new Int32Array(audioData[0].length * audioData.length);
							if (junkMode) {
								if (audioData.length === 2) {
									const aa = audioData[0];
									const ab = audioData[1];
									for (let i = 0, j = 0; i < int16Buffer.length; i += 2, j++) {
										int16Buffer[i] = (aa[j] * 32767) | 0;
										int16Buffer[i + 1] = (ab[j] * 32767) | 0;
									}
								} else {
									const aa = audioData[0];
									for (let i = 0; i < aa.length; i++) {
										int16Buffer[i] = (aa[i] * 32767) | 0;
									}
								}
							} else {
								if (audioData.length === 2) {
									const aa = audioData[0];
									const ab = audioData[1];
									for (let i = 0, j = 0; i < len; i += 4, j++) {
										int16Buffer[i] = (aa[j] * 32767) | 0;
										int16Buffer[i + 1] = (ab[j] * 32767) | 0;
										int16Buffer[i + 2] = int16Buffer[i];
										int16Buffer[i + 3] = int16Buffer[i + 1];
									}
								} else {
									const aa = audioData[0];
									for (let i = 0, j = 0; i < aa.length; i++, j += 2) {
										int16Buffer[j] = (aa[i] * 32767) | 0;
										int16Buffer[j + 1] = int16Buffer[j];
									}
								}
							}
							encodedStatus = Flac.FLAC__stream_encoder_process_interleaved(
								flac_encoder,
								int16Buffer,
								audioData[0].length
							);
						}
						if (!encodedStatus) {
							throw new Error("FLAC encoding failed");
						}
						const finishStatus = Flac.FLAC__stream_encoder_finish(flac_encoder);
						if (!finishStatus) {
							throw new Error("FLAC finalization failed");
						}
						Flac.FLAC__stream_encoder_delete(flac_encoder);
						const totalLength = encBuffer.reduce((sum, chunk) => sum + chunk.length, 0);
						const flacData = new Uint8Array(totalLength);
						let offset = 0;
						for (const chunk of encBuffer) {
							flacData.set(chunk, offset);
							offset += chunk.length;
						}
						const blob = new Blob([flacData], { type: "audio/flac" });
						downloadFile(blob, ".flac");
					} catch (err) {
						alert("The FLAC encoding failed: " + err.message);
						console.error(err);
					}
					exportBtn5.disabled = false;
				}
				v.appendChild(document.createElement("br"));
				const exportBtn6 = document.createElement("button");
				v.appendChild(exportBtn6);
				exportBtn6.textContent = "Export as ASAF (Serena, Original Library)";
				exportBtn6.onclick = async function() {
					exportBtn6.disabled = true;
					try {
						alert("Thank you for using our original custom audio format!\n\nThis new audio codec is meant to compete with our older, slower bloated ones, LSAC and FBAC. ASAF does this by dynamically choosing the compression algorithms that best fit the audio. We estimate that ASAF would produce files that are 40% to 98% smaller than LSAC and FBAC. Plus, ASAF audio has much less static and supports stereo audio!");
						const encoder = await ASAFCodecModule();
						encoder.init();
						const CHANNELS = exporters.length;
						const SAMPLERATE = exporters[0].sampleRate;
						const CHUNKS = Number(prompt("What is the number of chunks the encoder would use during audio analysis & encoding? (3200 = recommended, 1200 = bad, 2400 = okay, 2800 = good, 4800 = good, 9000 = bad") ?? 3200);
						const audioData = exporters.map(i => timelineBeginAudio ? i.audioData.subarray(timelineBeginAudio, timelineEndAudio) : i.audioData);
						const len = audioData[0].length * audioData.length;
						let int16Buffer = new Int16Array(len);
						if (audioData.length === 2) {
							const aa = audioData[0];
							const ab = audioData[1];
							for (let i = 0, j = 0; i < int16Buffer.length; i += 2, j++) {
								int16Buffer[i] = (aa[j] * 32767) | 0;
								int16Buffer[i + 1] = (ab[j] * 32767) | 0;
							}
						} else {
							const aa = audioData[0];
							for (let i = 0; i < aa.length; i++) {
								int16Buffer[i] = (aa[i] * 32767) | 0;
							}
						}
						const encoded = encoder.Encode(int16Buffer, SAMPLERATE, CHANNELS, CHUNKS, 0);
						const blob = new Blob([encoded], { type: "audio/serena" });
						downloadFile(blob, ".asaf");
					} catch (err) {
						alert("The ASAF encoding failed: " + err.message);
						console.error(err);
					}
					exportBtn5.disabled = false;
				}
				window.onscroll = function() {
					const scrx = window.scrollX;
					apply.style.marginLeft = scrx + "px";
					opt.style.marginLeft = apply.style.marginLeft;
					params.style.marginLeft = apply.style.marginLeft;
					selectLabel.style.marginLeft = apply.style.marginLeft;
					metadataOfAudio.style.marginLeft = apply.style.marginLeft;
					renameFile.style.marginLeft = apply.style.marginLeft;
					bpm.style.marginLeft = apply.style.marginLeft;
					exportBtn.style.marginLeft = apply.style.marginLeft;
					exportBtn3.style.marginLeft = apply.style.marginLeft;
					exportBtn6.style.marginLeft = apply.style.marginLeft;
					play.style.marginLeft = apply.style.marginLeft;
					selectLabel.style.wordWrap = "keep-all";
					selectLabel.style.whiteSpace = "nowrap";
					volume.style.marginLeft = apply.style.marginLeft;
					volume2.style.marginLeft = apply.style.marginLeft;
					subtitlesDiv.style.top = ["calc(70vh + ", window.scrollY, "px)"].join("");
					subtitlesDiv.style.left = [scrx, "px"].join("");
					if (window.scrollX > window.innerWidth - (selectLabel.getBoundingClientRect().width + selectStart.getBoundingClientRect().width + selectEnd.getBoundingClientRect().width + 14)) {
						selectBox.style.marginLeft = apply.style.marginLeft;
						selectStart.style.marginLeft = apply.style.marginLeft;
						selectEnd.style.marginLeft = apply.style.marginLeft;
					} else {
						selectBox.style.marginLeft = "";
						selectStart.style.marginLeft = "";
						selectEnd.style.marginLeft = "";
					}
					if (skipSample === 1) {
						count++;
						if (count < 4) return;
						count = 0;
						let data2 = ["M 0,100 L 0,100"];
						let xn = window.scrollX;
						x.setAttribute("width", window.innerWidth);
						x.style.marginLeft = (xn - xOff) + "px";
						const len = Math.min(xn - xOff + window.innerWidth, yu), y = exp[0].audioData;
						let prev = 0, val = y[0];
						let ep = 0.004;
						for (let i = xn, j = 0; i < len; i++, j++) {
							if (Math.abs(prev - val) >= ep) {
								data2.push("L " + j + "," + Math.round(100 + val * 100));
								prev = val;
							}
							val = y[i];
						}
						data2.push("L " + (len / skipSample) + ",100");
						n.setAttribute("d", data2.join(" "));
						if (n2) {
							data2 = ["M 0,100 L 0,100"];
							const y = exp[1].audioData;
							let prev = 0, val = y[0];
							for (let i = xn, j = 0; i < len; i++, j++) {
								if (Math.abs(prev - val) >= ep) {
									data2.push("L " + j + "," + Math.round(100 + val * 100));
									prev = val;
								}
								val = y[i];
							}
							data2.push("L " + (len / skipSample) + ",100");
							n2.setAttribute("d", data2.join(" "));
						}
					}
				}
				document.body.onclick = function(event) {
					canUseSpaceKey = event.target !== loadFromURL;
				}
				effects["trim"] = function(ex) {
					if (timelineBeginAudio) ex.audioData = ex.audioData.subarray(timelineBeginAudio, timelineEndAudio); else alert("You have not selected any portion of the audio yet.");
				}
				effects["trimout"] = function(ex) {
					if (timelineBeginAudio) ex.audioData = new Float32Array([]); else alert("You have not selected any portion of the audio yet.");
				}
				ed = v;
				div = x;
			}
			effectsList.push([
				"Trim",
				"Removes all of the contents that is not selected by the blue selection box.",
				'',
				0,
				"trim",
				[]
			]);
			effectsList.push([
				"Trim Out",
				"Removes the contents that are selected by the blue selection box.",
				'',
				0,
				"trimout",
				[]
			]);
			function processSharedBuff(channel, channelNum, isProcessed) {
				if (sharedBuffer.numberOfChannels === 4) {
					const which = confirm("Do you want the surround sound audio to be converted into mono (Cancel/No) or stereo (OK/Yes)?");
					if (which) {
						const newBuffer = audioContext.createBuffer(2, sharedBuffer.length, sharedBuffer.sampleRate);
						const lenBuffer = sharedBuffer.length;
						const channel1 = newBuffer.getChannelData(0);
						const channel2 = newBuffer.getChannelData(1);
						const channel1S = sharedBuffer.getChannelData(0);
						const channel2S = sharedBuffer.getChannelData(1);
						const channel3S = sharedBuffer.getChannelData(2);
						const channel4S = sharedBuffer.getChannelData(3);
						const leftorright = confirm("Okay! Do you want to store the front left and right sides (Cancel/No), or all sides including the front left, front right, back left and back right sides (OK/Yes)?");
						if (leftorright) {
							for (let i = 0; i < lenBuffer; i++) {
								channel1[i] = (channel1S[i] + channel3S[i]) * 0.5;
								channel2[i] = (channel2S[i] + channel4S[i]) * 0.5;
							}
						} else {
							for (let i = 0; i < lenBuffer; i++) {
								channel1[i] = channel1S[i];
								channel2[i] = channel2S[i];
							}
						}
						sharedBuffer = newBuffer;
						channel = sharedBuffer.getChannelData(channelNum & 1);
					} else {
						const newBuffer = audioContext.createBuffer(1, sharedBuffer.length, sharedBuffer.sampleRate);
						const lenBuffer = sharedBuffer.length;
						const channel1 = newBuffer.getChannelData(0);
						const channel1S = sharedBuffer.getChannelData(0);
						const channel2S = sharedBuffer.getChannelData(1);
						const channel3S = sharedBuffer.getChannelData(2);
						const channel4S = sharedBuffer.getChannelData(3);
						const leftorright = confirm("Okay! Do you want to store the left and right sides (Cancel/No), or all sides including the left, right, front and back sides (OK/Yes)?");
						if (leftorright) {
							for (let i = 0; i < lenBuffer; i++) channel1[i] = (channel1S[i] + channel2S[i] + channel3S[i] + channel4S[i]) * 0.25;
						} else {
							for (let i = 0; i < lenBuffer; i++) channel1[i] = (channel1S[i] + channel2S[i]) * 0.5;
						}
						sharedBuffer = newBuffer;
						channel = sharedBuffer.getChannelData(0);
					}
				} else if (sharedBuffer.numberOfChannels === 8) {
					const open = !confirm("The editor will assume that this surround sound audio follows specific conventions. This may not be important, but it will play a role in rendering audio into stereo. You may view the page in another tab if you press No or Cancel.");
					const which = confirm("Do you want the 7.1 surround sound audio to be converted into mono (Cancel/No) or stereo (OK/Yes)?");
					if (which) {
						const newBuffer = audioContext.createBuffer(2, sharedBuffer.length, sharedBuffer.sampleRate);
						const lenBuffer = sharedBuffer.length;
						const channel1 = newBuffer.getChannelData(0);
						const channel2 = newBuffer.getChannelData(1);
						const channel1S = sharedBuffer.getChannelData(0);
						const channel2S = sharedBuffer.getChannelData(1);
						const channel3S = sharedBuffer.getChannelData(2);
						const channel4S = sharedBuffer.getChannelData(3);
						const channel5S = sharedBuffer.getChannelData(4);
						const channel6S = sharedBuffer.getChannelData(5);
						const channel7S = sharedBuffer.getChannelData(6);
						const channel8S = sharedBuffer.getChannelData(7);
						const leftorright = confirm("Okay! Do you want to keep all the centered channels (OK/Yes), or do you want to discard them (Cancel/No)?");
						const xy = 1/5.4;
						if (leftorright) {
							for (let i = 0; i < lenBuffer; i++) {
								channel1[i] = (channel1S[i]*0.75 + channel3S[i] + channel4S[i]*2 + channel5S[i]*0.65 + channel7S[i]) * xy;
								channel2[i] = (channel2S[i]*0.75 + channel3S[i] + channel4S[i]*2 + channel6S[i]*0.65 + channel8S[i]) * xy;
							}
						} else {
							for (let i = 0; i < lenBuffer; i++) {
								channel1[i] = (channel1S[i]*0.75 + channel5S[i]*0.65 + channel7S[i]) * 0.4166666666666667;
								channel2[i] = (channel2S[i]*0.75 + channel6S[i]*0.65 + channel8S[i]) * 0.4166666666666667;
							}
						}
						sharedBuffer = newBuffer;
						channel = sharedBuffer.getChannelData(channelNum & 1);
					} else {
						const newBuffer = audioContext.createBuffer(1, sharedBuffer.length, sharedBuffer.sampleRate);
						const lenBuffer = sharedBuffer.length;
						const channel1 = newBuffer.getChannelData(0);
						const channel1S = sharedBuffer.getChannelData(0);
						const channel2S = sharedBuffer.getChannelData(1);
						const channel3S = sharedBuffer.getChannelData(2);
						const channel4S = sharedBuffer.getChannelData(3);
						const channel5S = sharedBuffer.getChannelData(4);
						const channel6S = sharedBuffer.getChannelData(5);
						const channel7S = sharedBuffer.getChannelData(6);
						const channel8S = sharedBuffer.getChannelData(7);
						const leftorright = confirm("Okay! Do you want to keep all the centered channels (OK/Yes), or do you want to discard them (Cancel/No)?");
						if (leftorright) {
							for (let i = 0; i < lenBuffer; i++) channel1[i] = (channel1S[i] + channel2S[i] + channel3S[i]*2 + channel4S[i]*2 + channel5S[i] + channel6S[i] + channel7S[i] + channel8S[i]) * 0.1;
						} else {
							for (let i = 0; i < lenBuffer; i++) channel1[i] = (channel1S[i] + channel2S[i] + channel5S[i] + channel6S[i] + channel7S[i] + channel8S[i]) * 0.16666666666666666666;
						}
						sharedBuffer = newBuffer;
						channel = sharedBuffer.getChannelData(0);
					}
					if (open) window.open('https://unnamedbruh.github.io/newer-audio-editor/surroundsoundconventions-81.html', '_blank');
				}
				if (sharedBuffer.numberOfChannels === 2) {
					if (confirm("Should the current channel be subtracted by another? (If you want to only edit the selected channel, press Cancel/No)")) {
						const channelOtherNum = sharedBuffer.numberOfChannels - channelNum - 1;
						const len = channel.length;
						const channelOther = sharedBuffer.getChannelData(channelOtherNum);
						while (true) {
							const choice = Number(prompt("Okay then! Which formula do you want to choose? (-1 = exit this prompt | 0 = channel" + channelNum + " - otherChannel | 1 = channel" + channelNum + " * 2 - otherChannel | 2 = channel" + channelNum + " - (otherChannel - channel" + channelNum + ") / 2 | 3 = 2 but otherChannel" + channelNum + " * 2 | 4 = channel" + channelNum + " + otherChannel | 5 = channel" + channelNum + " * -1)"));
							if (choice === 0) for (let i = 0; i < len; i++) channel[i] -= channelOther[i];
							else if (choice === 1) for (let i = 0; i < len; i++) channel[i] = channelOther[i] - channel[i];
							else if (choice === 2) for (let i = 0; i < len; i++) channel[i] -= Math.abs(channelOther[i] - channel[i]); else if (choice === 3) for (let i = 0; i < len; i++) channel[i] -= channelOther[i] * 2 - channel[i];
							else if (choice === 3.5) for (let i = 0; i < len; i++) channel[i] -= channelOther[i] - channel[i] * 2;
							else if (choice === 4) for (let i = 0; i < len; i++) channel[i] = (channel[i] + channelOther[i]) * 0.5;
							else if (choice === 5) {
								for (let i = 0; i < len; i++) channel[i] = -channel[i];
								continue;
							} else if (choice === -1) break;
							isProcessed = true;
							break;
						}
					}
				}
				channel = sharedBuffer.getChannelData(Math.min(channelNum & 1, sharedBuffer.numberOfChannels-1));
				return [channel,isProcessed,sharedBuffer];
			}
			let oldSharedBufferFor = null;
			function copyBuffer(buff) {
				const copy = audioContext.createBuffer(buff.numberOfChannels, buff.length, buff.sampleRate);
				for (let i = 0; i < buff.numberOfChannels; i++) copy.copyToChannel(buff.getChannelData(i), i);
				return copy;
			}
			function loadSharedBuffer(sharedBuffer) {
				const channelNum = Math.max(1, Math.min(+prompt("Which channel should be imported for editing? Pick a channel from 1 to " + sharedBuffer.numberOfChannels + ". (Editing multiple channels at once is now officially an option due to the maintainer's goals! However, this only works on stereo audio, not surround-sound.)") || 1, sharedBuffer.numberOfChannels)) - 1;
				let channel = sharedBuffer.getChannelData(channelNum);
				let isProcessed = false;
				if (sharedBuffer.numberOfChannels > 2 && confirm("Do you wish to select only that channel?")) {
					const newBuffer = audioContext.createBuffer(1, sharedBuffer.length, sharedBuffer.sampleRate);
					newBuffer.copyToChannel(channel, 0);
					sharedBuffer = newBuffer;
					channel = sharedBuffer.getChannelData(0);
				} else {const x = processSharedBuff(channel, channelNum, isProcessed);isProcessed = x[1];channel = x[0];sharedBuffer = x[2];}
				const stereo = sharedBuffer.numberOfChannels >= 2 && !isProcessed && confirm("Do you want the final audio to be stereo?");
				if (sharedBuffer.numberOfChannels >= 2 && !stereo) { // Normalize playback, since only one channel is being loaded. If other channels are playing while one is selected, it can suddenly sound "mono" if an effect is applied, which confuses the user
					for (let i = 0; i < sharedBuffer.numberOfChannels; i++) {
						if (i === channelNum) continue;
						const silentBuffer = sharedBuffer.getChannelData(i);
						if (silentBuffer) silentBuffer.set(channel);
					}
				}
				if (stereo) exporters = [new AudioExporter(channel, sharedBuffer.sampleRate, 1, 32), new AudioExporter(sharedBuffer.getChannelData(1 - channelNum), sharedBuffer.sampleRate, 1, 32)];
				else exporters = [new AudioExporter(channel, sharedBuffer.sampleRate, 1, 32)];
				loadDiv(exporters);
				return sharedBuffer;
			}
			async function loadAudioFiles(eve) {
				if (!eve || !eve.target || !eve.target.files || !eve.target.files.length) return;let firstFile = eve.target.files[0], secondFile = eve.target.files[1], isCorrect;
				function swapFiles() {const temp = secondFile;secondFile = firstFile;firstFile = temp;}
				if (secondFile) {
					isCorrect = confirm("It seems like you selected two audio files. Is this correct?");
					if (isCorrect) alert("*This editor cannot interpret the second file as raw audio because the maintainer of this editor has not implemented this feature... yet." + (whichInterpret ? " Not that the audio won't be decoded." : ""));
					if (!confirm("Which file is the one you want to edit?\nOK = " + firstFile.name.trim() + "\nCancel (or X) = " + secondFile.name.trim())) swapFiles();
				}
				if (!whichInterpret && (firstFile.name.endsWith(".lsac") || firstFile.name.endsWith(".fbac"))) {
					alert("LSAC and FBAC file loading is no longer supported by this audio editor, due to several issues with the original encoders and decoders, and lack of widespread support across many environments and plugins. We sincerely apologize for taking down our own audio format, but we hope to bring in a much better codec!");
				} else {
					const fileReader = new FileReader();
					fileReader.onload = async function(event) {
						try {
							exporters = null;
							if (ed) {
								ed.innerHTML = "";
								ed.remove();
								ed = null;
							}
							data = null;
							sampleData = null;
							skipSample = 480;
							if (whichInterpret) {
								const data = event.target.result;
								const type = Number(prompt("How will the audio be interpreted? (0 = integers, 1 = floats)"));
								const bits = Math.max(prompt("What is the bits per sample of the audio?") >> 3, 1);
								sharedBuffer = audioContext.createBuffer(1, data.byteLength / bits, prompt("What is the samplerate of the audio?") || 48000);
								const channel = sharedBuffer.getChannelData(0);
								const len = sharedBuffer.length;
								const buffer = new Uint8Array(data);
								if (type === 0) {
									if (bits === 1) {
										const bufferInt = new Int8Array(data);
										for (let i = 0; i < len; i++) channel[i] = bufferInt[i] / 0x80;
									} else if (bits === 2) {
										for (let i = 0, j = 0; i < len; i += bits, j++) channel[j] = ((buffer[i + 1] << 8) | buffer[i]) / 0x8000 - 0.5;
									} else if (bits === 3) {
										for (let i = 0, j = 0; i < len; i += bits, j++) channel[j] = ((buffer[i + 2] << 16) | (buffer[i + 1] << 8) | buffer[i]) / 0x800000 - 0.5;
									} else if (bits === 4) {
										for (let i = 0, j = 0; i < len; i += bits, j++) channel[j] = ((buffer[i + 3] << 24) | (buffer[i + 2] << 16) | (buffer[i + 1] << 8) | buffer[i]) / 0x80000000 - 0.5;
									}
								} else if (type === 1) {
									const s = Math.sign;
									if (bits === 4) {
										const f = new Float32Array(data.slice(0, data.byteLength >> 2 << 2));
										const lenF = f.length;
										let x;
										for (let i = 0; i < lenF; i++) {
											x = f[i];
											channel[i] = (x === Infinity ? 1 : (x === -Infinity ? -1 : (isNaN(x) ? 0 : x))) || 0;channel[i] = channel[i] > 1 ? 1 : (channel[i] < -1 ? -1 : channel[i]);
										}
									} else if (bits === 8) {
										const f = new Float64Array(data.slice(0, data.byteLength >> 3 << 3));
										const lenF = f.length;
										let x;
										for (let i = 0; i < lenF; i++) {
											x = f[i];
											channel[i] = (x === Infinity ? 1 : (x === -Infinity ? -1 : (isNaN(x) ? 0 : x))) || 0;channel[i] = channel[i] > 1 ? 1 : (channel[i] < -1 ? -1 : channel[i]);
										}
									}
								}
								oldSharedBufferFor = copyBuffer(sharedBuffer);
								exporters = [new AudioExporter(channel, sharedBuffer.sampleRate, 1, 32)];
								filename = "My File Interpreted As Raw Audio";
								loadDiv(exporters);
								importSubtitles.disabled = false;
								reimp.disabled = false;
								return;
							}
							loadFromURL.style.color = "green";
							const oldURL = loadFromURL.value;
							loadFromURL.value = "success!";
							sharedBuffer = await audioContext.decodeAudioData(event.target.result);
							if (isCorrect) {
								alert("The first audio file was decoded.");
								const fileReader2 = new FileReader();
								let secondSharedBuffer;
								await new Promise(function(resolve) {
									try {
										fileReader2.onload = async function(eve) {
											try {
												secondSharedBuffer = await audioContext.decodeAudioData(eve.target.result);
											} catch {
												alert("Looks like the second audio file has failed to be decoded! However, you can still edit the main audio.");
											}
											if (secondSharedBuffer && confirm("Would you like to layer both audio files together?")) {
												let inway = Number(prompt("Alright! Which way should the audio editor layer the audio files together? (0 = audioFile1 + audioFile2, 1 = audioFile1 - audioFile2, 2 = audioFile2 - audioFile1") || 0);
												if (sharedBuffer.sampleRate === secondSharedBuffer.sampleRate && sharedBuffer.numberOfChannels === secondSharedBuffer.numberOfChannels) {
													let len;
													if (sharedBuffer.length < secondSharedBuffer.length) {
														len = secondSharedBuffer.length;
														const oldShared = sharedBuffer;sharedBuffer = audioContext.createBuffer(oldShared.numberOfChannels, len, oldShared.sampleRate);sharedBuffer.copyToChannel(oldShared.getChannelData(0), 0);
														if (sharedBuffer.numberOfChannels >= 2) sharedBuffer.copyToChannel(oldShared.getChannelData(1), 1);
													} else if (secondSharedBuffer.length < sharedBuffer.length) {
														len = sharedBuffer.length;
														const oldShared = secondSharedBuffer;secondSharedBuffer = audioContext.createBuffer(oldShared.numberOfChannels, len, oldShared.sampleRate);secondSharedBuffer.copyToChannel(oldShared.getChannelData(0), 0);
														if (sharedBuffer.numberOfChannels >= 2) secondSharedBuffer.copyToChannel(oldShared.getChannelData(1), 1);
													} else len = sharedBuffer.length;
													if (inway === 0) {
														for (let i = 0; i < secondSharedBuffer.numberOfChannels; i++) {
															const pointer1 = sharedBuffer.getChannelData(i);
															const pointer2 = secondSharedBuffer.getChannelData(i);
															for (let j = 0; j < len; j++) pointer1[j] += pointer2[j];
														}
													} else if (inway === 1 || inway === 2 || inway === 3 || inway === 4) {
														if (inway === 3 || inway === 4) {
															for (let i = 0; i < secondSharedBuffer.numberOfChannels; i++) {
																const pointer = (inway === 3 ? sharedBuffer.getChannelData(i) : secondSharedBuffer.getChannelData(i));
																for (let j = 0; j < len; j++) pointer[j] *= 0.5;
															}
															inway -= 2;
														}
														for (let i = 0; i < secondSharedBuffer.numberOfChannels; i++) {
															let pointer1;
															let pointer2;
															if (inway === 1) {
																pointer1 = sharedBuffer.getChannelData(i);
																pointer2 = secondSharedBuffer.getChannelData(i);
															} else {
																pointer2 = sharedBuffer.getChannelData(i);
																pointer1 = secondSharedBuffer.getChannelData(i);
															}
															for (let j = 0; j < len; j++) pointer1[j] -= pointer2[j];
														}
													}
												} else alert("Looks like the audio files' metadata doesn't exactly match each other well! You can still edit the main audio, though.");
											}
											resolve();
										}
										fileReader2.readAsArrayBuffer(secondFile);
									} catch {
										alert("Looks like something went wrong in the mixing process! However, you can still edit the main audio.");
										resolve();
									}
								});
							}
							oldSharedBufferFor = copyBuffer(sharedBuffer);
							filename = firstFile.name.split(".").slice(0, -1).join(".");
							sharedBuffer = loadSharedBuffer(sharedBuffer);
							importSubtitles.disabled = false;
							reimp.disabled = false;
							loadFromURL.disabled = false;
							loadFromURL.value = oldURL;
							loadFromURL.style.color = "";
						} catch (err) {
							alert("The audio could not be properly loaded! We couldn't gather any info on what exactly failed, so the best we can say is: try selecting another audio file, or using another browser.");
							console.log(err.message + err.stack);
							loadFromURL.disabled = false;
							loadFromURL.style.color = "";
						}
					}
					fileReader.readAsArrayBuffer(firstFile);
				}
			}
			audioInput.oninput = async event => loadAudioFiles(event);
			loadBlank.onclick = function() {
				exporters = null;
				if (ed) {
					ed.innerHTML = "";
					ed.remove();
					ed = null;
				}
				data = null;
				sampleData = null;
				skipSample = 480;
				sharedBuffer = audioContext.createBuffer(1 + confirm("Will the blank audio be mono (1 channel, select \"Cancel\") or stereo (2 channels, select \"OK\" or \"Yes\")?"), (Math.abs(Number(prompt("How long is the blank audio (in seconds)?"))) || 1) * audioContext.sampleRate, audioContext.sampleRate);
				const stereo = sharedBuffer.numberOfChannels === 2;
				if (stereo) exporters = [new AudioExporter(sharedBuffer.getChannelData(0), sharedBuffer.sampleRate, 1, 32), new AudioExporter(sharedBuffer.getChannelData(1), sharedBuffer.sampleRate, 1, 32)];
				else exporters = [new AudioExporter(sharedBuffer.getChannelData(0), sharedBuffer.sampleRate, 1, 32)];
				filename = "My Blank Audio";
				loadDiv(exporters, true);
				reimp.disabled = false;
				reimp.textContent = "Re-import Audio Before This Blank Audio";
				importSubtitles.disabled = false;
			}
			reimp.onclick = function() {
				let x = copyBuffer(oldSharedBufferFor);
				sharedBuffer = oldSharedBufferFor;
				oldSharedBufferFor = x;
				exporters = null;
				if (ed) {
					ed.innerHTML = "";
					ed.remove();
					ed = null;
				}
				data = null;
				sampleData = null;
				skipSample = 480;
				sharedBuffer = loadSharedBuffer(sharedBuffer);
			}
			let isRecording = false, recordedBuffers = null, stream = null, processor = null, source = null, mergedBuffers = false, silentNode = null;
			recordMicrophone.onclick = async function() {
				isRecording = !isRecording;
				if (typeof MediaRecorder !== "undefined") { // All of this block was written by Grok's non-Premium model. I altered the code to add compatibility fallbacks, and tweaking how the button is handled
					if (isRecording) {
						recordMicrophone.textContent = "Asking Permission...";
					recordMicrophone.disabled = true;
		try {
			stream = await navigator.mediaDevices.getUserMedia({
				audio: { channelCount: 2, sampleRate: audioContext.sampleRate }
			});
			if (MediaRecorder.isTypeSupported('audio/webm;codecs=pcm')) mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=pcm' }); // Try for PCM if supported
			else if (MediaRecorder.isTypeSupported('audio/webm')) mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
			recordedChunks = [];
			mediaRecorder.ondataavailable = (e) => {if (e.data.size > 0) recordedChunks.push(e.data);}
			mediaRecorder.onstop = async () => {
				const blob = new Blob(recordedChunks, { type: 'audio/webm' });
				const arrayBuffer = await blob.arrayBuffer();
				sharedBuffer = await audioContext.decodeAudioData(arrayBuffer);
				oldSharedBufferFor = copyBuffer(sharedBuffer);
				exporters = null;
				if (ed) {
					ed.innerHTML = "";
					ed.remove();
					ed = null;
				}
				data = null;
				sampleData = null;
				skipSample = 480;
				sharedBuffer = loadSharedBuffer(sharedBuffer);
				stream.getTracks().forEach(t => t.stop());
				recordMicrophone.textContent = "Import Recording From Microphone";
				recordMicrophone.disabled = false;
				importSubtitles.disabled = false;
				reimp.disabled = false;
			};
			mediaRecorder.start();
			recordMicrophone.textContent = "Stop Recording";
			recordMicrophone.disabled = false;
		} catch {
			recordMicrophone.textContent = "Microphone cannot be accessed";
		}
	} else {
		mediaRecorder.stop();
		recordMicrophone.textContent = "Processing...";
		recordMicrophone.disabled = true;
	}
				} else {if (isRecording) { // This block was written by GPT-5.2 Mini.
					recordMicrophone.textContent = "Asking Permission...";
					recordMicrophone.disabled = true;
					try{
						stream = await navigator.mediaDevices.getUserMedia({ audio: {channelCount: 2, sampleRate: audioContext.sampleRate} });
						recordMicrophone.textContent = "Setting things up...";
						source = audioContext.createMediaStreamSource(stream);
						const bufferSize = 4096; // typical buffer size
						processor = audioContext.createScriptProcessor(bufferSize, source.channelCount, source.channelCount);
						recordedBuffers = Array.from({length: source.channelCount}, () => []);
						processor.onaudioprocess = (event) => {
							console.log(event);
							for (let ch = 0; ch < source.channelCount; ch++) recordedBuffers[ch].push(new Float32Array(event.inputBuffer.getChannelData(ch)));
						}
						source.connect(processor);
						silentNode = audioContext.createGain();
						silentNode.gain.value = 0.3;
						processor.connect(silentNode);
						silentNode.connect(audioContext.destination); // To guarantee that the audio recording will be pushed
						recordMicrophone.textContent = "Stop Recording";
						recordMicrophone.disabled = false;
					}catch{
						recordMicrophone.textContent = "Microphone cannot be accessed";
					}
				} else {
					silentNode.disconnect();
					processor.disconnect();
					source.disconnect();
					stream.getTracks().forEach(track => {
						track.stop();
					});
					recordMicrophone.textContent = "Merging chunks...";
					recordMicrophone.disabled = true;
					silentNode = null;
					function mergeBuffers(buffers) {
						let length = 0;
						for (const buf of buffers) {
							length += buf.length;
						}
						const result = new Float32Array(length);
						let offset = 0;
						for (const buf of buffers) {
							result.set(buf, offset);
							offset += buf.length;
						}
						return result;
					}
					exporters = null;
					if (ed) {
						ed.innerHTML = "";
						ed.remove();
						ed = null;
					}
					data = null;
					sampleData = null;
					skipSample = 480;
					mergedBuffers = recordedBuffers.map(x => mergeBuffers(x));
					sharedBuffer = audioContext.createBuffer(mergedBuffers.length, mergedBuffers[0].length, audioContext.sampleRate);
					for (let i = 0; i < mergedBuffers.length; i++) {
						sharedBuffer.copyToChannel(mergedBuffers[i], i);
					}
					oldSharedBufferFor = copyBuffer(sharedBuffer);
					loadSharedBuffer(sharedBuffer);
					recordMicrophone.textContent = "Import Recording From Microphone";
					recordMicrophone.disabled = false;
					importSubtitles.disabled = false;
					reimp.disabled = false;
				}}
			}
			loadFromURL.onkeydown = async event => {
				const obj = {};
				obj.target = {files: []};
				requestAnimationFrame(()=>{loadFromURL.style.width = (loadFromURL.value.length * 8) + "px";});
				if (event.key === "Enter") {
					loadFromURL.disabled = true;
					const urls = new Set(loadFromURL.value.split(",")).values().toArray();
					for (let i = 0, url = urls[0]; i < urls.length; url = urls[++i]) {
						try {
							const firstFile = await originalFetchFunction(`https://api.codetabs.com/v1/proxy/?quest=${encodeURIComponent(url.trim())}`);
							const arrbuff = await firstFile.arrayBuffer();
							if (!arrbuff || !arrbuff.byteLength) {
								loadFromURL.disabled = false;
								alert("URL #" + (i+1) + " was unable to be fetched. This is not an error, but it's something important to keep in mind.");
								continue;
							}
							obj.target.files.push(new File([arrbuff], decodeURIComponent(loadFromURL.value.split("/").slice(-1)[0])));
						} catch {}
					}
					await loadAudioFiles(obj);
				}
			}
			const line = document.getElementById("b");
			const header = document.getElementById("disappearheader");
			function changeYPositionOfLine() {
				const bounds = line.getBoundingClientRect();
				line.style.marginTop = (window.innerHeight - header.getBoundingClientRect().height) + "px";
			}
			setTimeout(function() {
				header.classList.add("shrinkheader");
				let ccc = true;
				function keep() {changeYPositionOfLine();if(ccc)requestAnimationFrame(()=>keep());}
				keep();
				setTimeout(function() {
					header.outerHTML = "<br>";
					ccc = false;
				}, 1500)
			}, 3000);
			changeYPositionOfLine();
		</script>
	</body>
</html>
